{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347725a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "이 폴더 한번 싹 훑어보고 f1 macro의 점수를 좀더 높게 받으려면 어떤부분을 개선해야할지 한번 말해줄수있어?\n",
    "\n",
    "Gemini\n",
    "안녕하세요! Gemini Code Assist입니다. 제출하신 전체 코드를 꼼꼼하게 검토했습니다. 이미 K-Fold 교차 검증, TTA, 앙상블 등 매우 수준 높은 기법들을 잘 활용하고 계십니다. F1 Macro 점수를 한 단계 더 끌어올리기 위한 몇 가지 개선 방안을 제안해 드립니다.\n",
    "\n",
    "종합 분석 및 개선 방향\n",
    "현재 프로젝트는 다음과 같은 강점을 가지고 있습니다.\n",
    "\n",
    "상세한 EDA: korean_document_eda.py를 통해 데이터 불균형, 이미지 크기 등 핵심 특징을 잘 파악하고 있습니다.\n",
    "체계적인 훈련: main_test.py에서 StratifiedKFold를 사용하여 안정적인 검증을 수행합니다.\n",
    "고급 추론 기법: inference.py와 inference_ensemble.py에서 TTA와 다중 모델 앙상블을 구현하여 성능을 극대화하고 있습니다.\n",
    "이러한 강점을 바탕으로, F1 Macro 점수를 높이기 위해 소수 클래스(minority class)의 성능을 개선하고 모델의 일반화 성능을 극대화하는 데 초점을 맞춘 개선안을 제안합니다.\n",
    "\n",
    "🎯 개선 제안 (F1 Macro 점수 향상)\n",
    "1. K-Fold 교차 검증 결과물을 활용한 앙상블 (가장 효과적인 방법)\n",
    "현재 main_test.py에서 K-Fold 훈련을 하고 있지만, 각 Fold에서 생성된 모델을 앙상블에 직접 활용하고 있지 않습니다. 각 Fold의 최고 성능 모델들을 앙상블하면 단일 모델이나 서로 다른 아키텍처를 앙상블하는 것보다 더 안정적이고 높은 성능을 기대할 수 있습니다.\n",
    "\n",
    "실행 방안:\n",
    "\n",
    "main_test.py를 실행하여 각 Fold의 best-checkpoint를 모두 저장합니다. (EfficientNetV2M_fold0_....pth, EfficientNetV2M_fold1_....pth, ...)\n",
    "inference_ensemble.py를 수정하여 이 모델들을 모두 불러와 추론하고, 결과의 평균을 사용합니다.\n",
    "inference_ensemble.py 수정 제안:\n",
    "\n",
    "inference_ensemble.py\n",
    "-20\n",
    "+11\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from datasets.transforms import build_unified_transforms\n",
    "from utils.predict_tta import get_tta_predictions\n",
    "\n",
    "\n",
    "# --- 기본 설정 ---\n",
    "cfg = load_config(\"config/inference_config.yaml\")\n",
    "ModelClass_A = get_model(cfg['MODEL_A']['name'])\n",
    "ModelClass_B = get_model(cfg['MODEL_B']['name'])\n",
    "ModelClass_C = get_model(cfg['MODEL_C']['name'])\n",
    "ModelClass_D = get_model(cfg['MODEL_D']['name'])\n",
    "ModelClassList = [\n",
    "    ModelClass_A,\n",
    "    ModelClass_B,\n",
    "    ModelClass_C,\n",
    "]\n",
    "# K-Fold 앙상블을 위해 단일 모델 클래스를 사용합니다.\n",
    "ModelClass = get_model(cfg['MODEL']) # config에 앙상블할 모델 이름 추가 필요 (예: 'EfficientNetV2MArcFaceModel')\n",
    "DatasetClass = get_dataset(cfg['DATASET'])\n",
    "num_classes = 17\n",
    "num_workers = os.cpu_count() // 2\n",
    "\n",
    "# --- 앙상블 설정 ---\n",
    "# 1. 사용할 모델들의 체크포인트 경로 리스트\n",
    "model_paths = [\n",
    "    f\"{output_root}/EfficientNetV2MArcFaceModel_2025-07-05_20-40_checkpoint.pth\",\n",
    "    f\"{output_root}/ResNeStModelArcFaceModel_2025-07-06_03-13_checkpoint.pth\",\n",
    "    f\"{output_root}/ConvNeXtArcFaceModel_2025-07-06_05-22_checkpoint.pth\",\n",
    "]\n",
    "# K-Fold로 생성된 모든 모델 체크포인트를 동적으로 불러옵니다.\n",
    "# 예시: \"EfficientNetV2MArcFaceModel_fold*_checkpoint.pth\" 패턴으로 파일 검색\n",
    "model_pattern = f\"{cfg['MODEL']}_fold*_checkpoint.pth\" \n",
    "model_paths = glob.glob(os.path.join(output_root, model_pattern))\n",
    "print(f\"발견된 K-Fold 모델 {len(model_paths)}개: {model_paths}\")\n",
    "\n",
    "# 가중치 리스트 정의\n",
    "# 모델 경로(model_paths) 순서와 동일하게 가중치를 설정합니다.\n",
    "num_models = len(model_paths)\n",
    "use_weight = False\n",
    "if use_weight:\n",
    "    weights = [0.2, 0.4, 0.4]\n",
    "    # weights = [0.2, 0.3, 0.3, 0.2]\n",
    "    weights = [0.2, 0.2, 0.2, 0.2, 0.2] # Fold 개수에 맞게 수정\n",
    "else:\n",
    "    weights = [1 / num_models] * num_models\n",
    "\n",
    "# --- 모델 로드 ---\n",
    "ensemble_models = []\n",
    "print(\"앙상블 모델 로딩 시작...\")\n",
    "for path, ModelClass in zip(model_paths, ModelClassList):\n",
    "    # 주의: 모델 아키텍처가 다를 경우, get_model 등을 통해 각 모델에 맞는 클래스를 불러와야 합니다.\n",
    "    model = ModelClass(num_classes).to(device)\n",
    "for path in model_paths:\n",
    "    model = ModelClass(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval() # 평가 모드로 즉시 설정\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "2. 데이터 불균형 문제 심층 해결\n",
    "F1 Macro 점수는 각 클래스의 F1 점수의 산술 평균이므로, 데이터가 적은 클래스의 성능이 전체 점수에 큰 영향을 미칩니다. EDA에서 확인된 데이터 불균형을 해결하는 것이 점수 향상의 핵심입니다.\n",
    "\n",
    "실행 방안:\n",
    "\n",
    "Focal Loss 사용: main.py에서 get_loss 함수를 통해 손실 함수를 불러오고 있습니다. 일반적인 CrossEntropyLoss 대신 Focal Loss를 사용해 보세요. Focal Loss는 맞추기 어려운 샘플(주로 소수 클래스)에 더 큰 가중치를 부여하여 모델이 소수 클래스에 집중하도록 유도합니다. config.yaml에서 손실 함수 관련 설정을 변경하여 실험해 보시는 것을 추천합니다.\n",
    "\n",
    "yaml\n",
    "# config/main_config.yaml 예시\n",
    "loss:\n",
    "  name: \"FocalLoss\"\n",
    "  params:\n",
    "    alpha: 0.25\n",
    "    gamma: 2.0\n",
    "클래스별 가중치 적용: nn.CrossEntropyLoss를 사용하더라도, 클래스별 데이터 수의 역수에 비례하는 가중치(weight 인자)를 계산하여 적용할 수 있습니다. 이는 소수 클래스의 loss 영향력을 높여줍니다.\n",
    "\n",
    "3. 데이터 증강(Augmentation) 기법 고도화\n",
    "모델이 다양한 형태의 문서 이미지를 학습하게 하여 일반화 성능을 높일 수 있습니다.\n",
    "\n",
    "실행 방안:\n",
    "\n",
    "Mixup / CutMix 적용: 이미지와 레이블을 일정 비율로 섞는 Mixup이나 이미지의 일부를 다른 이미지로 대체하는 CutMix는 분류 모델의 성능과 강건함을 높이는 데 매우 효과적입니다. 훈련 루프(trainer.py의 training_loop) 내에서 배치를 받아 처리하는 부분에 적용할 수 있습니다.\n",
    "\n",
    "test_data_preprocess.py의 전처리 기법을 훈련에도 일부 적용: test_data_preprocess.py에서 사용한 왜곡 보정, 명암 조절 등의 기법들을 훈련 시 데이터 증강의 일부로 확률적으로 적용(e.g., transforms.RandomApply)하면 훈련 데이터와 테스트 데이터의 분포 차이를 줄여 성능 향상에 도움이 될 수 있습니다.\n",
    "\n",
    "4. 코드 리팩토링 및 관리 효율성 증대\n",
    "main.py 정리: 현재 main.py 파일에 DataLoader가 두 번 정의되어 있습니다. 중복되는 코드를 제거하여 가독성과 유지보수성을 높이는 것이 좋습니다.\n",
    "하드코딩된 경로 제거: inference_ensemble.py에 모델 경로가 하드코딩되어 있습니다. 위에서 제안한 것처럼 glob을 사용하거나, 훈련 스크립트에서 생성된 모델 경로들을 별도의 설정 파일이나 로그 파일에 기록하고 추론 시 이를 읽어오는 방식으로 자동화하는 것이 좋습니다.\n",
    "요약 및 다음 단계 제안\n",
    "가장 먼저 main_test.py를 활용한 K-Fold 훈련 및 앙상블(제안 1)을 시도해 보세요. 가장 큰 성능 향상을 기대할 수 있는 방법입니다.\n",
    "그다음, **Focal Loss를 적용(제안 2)**하여 소수 클래스의 성능을 개선하는 실험을 진행합니다.\n",
    "추가적인 성능 향상이 필요할 때 **Mixup/CutMix와 같은 고급 증강 기법(제안 3)**을 도입하는 것을 고려해 보세요.\n",
    "이 제안들이 F1 Macro 점수를 높이는 데 도움이 되기를 바랍니다. 성공적인 결과 있기를 응원합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65dabab",
   "metadata": {},
   "source": [
    "이 폴더 한번 싹 훑어보고 f1 macro의 점수를 좀더 높게 받으려면 어떤부분을 개선해야할지 한번 말해줄수있어?\n",
    "\n",
    "\n",
    "Gemini\n",
    "안녕하세요! Gemini Code Assist입니다. 제출하신 전체 코드를 꼼꼼하게 검토해 보았습니다. K-Fold 교차 검증, TTA, 앙상블 등 이미 훌륭한 기법들을 많이 적용하고 계시네요. 현재 구조에서 F1 Macro 점수를 더 높이기 위해 시도해 볼 만한 몇 가지 개선 방안을 제안해 드립니다.\n",
    "\n",
    "종합 평가 및 개선 방향\n",
    "F1 Macro 점수는 각 클래스에 동일한 가중치를 부여하여 평균을 내기 때문에, 데이터가 적은 클래스(minority class)의 성능이 전체 점수에 큰 영향을 미칩니다. 따라서 개선의 핵심은 **\"데이터 불균형 문제를 얼마나 잘 해결하고, 소수 클래스의 성능을 높이는가\"**에 있습니다.\n",
    "\n",
    "🎯 주요 개선 제안 (F1 Macro 점수 향상 초점)\n",
    "1. 클래스 불균형 문제 심층 해결: Focal Loss 도입\n",
    "현재 main.py에서 sampler를 사용하여 클래스 불균형을 일부 완화하고 있지만, 이것만으로는 부족할 수 있습니다. 특히 분류하기 어려운(hard-to-classify) 샘플에 더 집중하여 학습하는 Focal Loss를 도입하는 것을 강력히 추천합니다. Focal Loss는 맞추기 쉬운 다수 클래스(majority class) 샘플들의 loss 기여도를 낮추고, 맞추기 어려운 소수 클래스 샘플에 더 집중하게 만들어 F1 Macro 점수 향상에 매우 효과적입니다.\n",
    "\n",
    "main.py의 get_loss 함수를 사용하고 있으므로, loss_factory.py에 Focal Loss를 추가하고 main_config.yaml에서 손쉽게 변경할 수 있도록 구성하는 것이 좋습니다.\n",
    "\n",
    "예시 (loss_factory.py에 Focal Loss 추가):\n",
    "\n",
    "python\n",
    "# utils/loss_factory.py (가상 경로)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def get_loss(name, params):\n",
    "    if name == \"CrossEntropyLoss\":\n",
    "        return nn.CrossEntropyLoss(**params)\n",
    "    elif name == \"FocalLoss\":\n",
    "        return FocalLoss(**params)\n",
    "    # ... 다른 손실 함수들\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss function: {name}\")\n",
    "적용 (main_config.yaml 수정):\n",
    "\n",
    "yaml\n",
    "# config/main_config.yaml\n",
    "loss:\n",
    "  # name: \"CrossEntropyLoss\"\n",
    "  # params: {}\n",
    "  name: \"FocalLoss\"\n",
    "  params:\n",
    "    alpha: 0.25\n",
    "    gamma: 2.0\n",
    "2. K-Fold 교차 검증과 앙상블의 결합\n",
    "main_test.py에서 K-Fold를 구현하셨고, inference_ensemble.py에서 여러 모델을 앙상블하고 있습니다. 이 두 가지 강력한 기법을 결합하여 성능을 극대화할 수 있습니다.\n",
    "\n",
    "현재 방식:\n",
    "\n",
    "Model A, B, C를 각각 한 번씩 학습 -> 3개 모델 앙상블\n",
    "제안 방식:\n",
    "\n",
    "5-Fold 교차 검증을 사용한다고 가정\n",
    "Model A를 5-Fold로 학습 -> 5개의 Model A 체크포인트 생성\n",
    "Model B를 5-Fold로 학습 -> 5개의 Model B 체크포인트 생성\n",
    "Model C를 5-Fold로 학습 -> 5개의 Model C 체크포인트 생성\n",
    "총 15개의 모델을 모두 앙상블하여 최종 예측 수행\n",
    "이렇게 하면 데이터셋을 훨씬 효율적으로 활용하게 되어 모델의 일반화 성능이 크게 향상되고, 단일 모델의 분산(variance)을 줄여주어 최종 앙상블 결과가 훨씬 안정적이고 강력해집니다.\n",
    "\n",
    "3. 테스트 데이터 전처리 로직 오류 수정\n",
    "test_data_preprocess.py 스크립트에서 심각한 논리적 오류가 발견되었습니다. 테스트 이미지에 어떤 전처리를 적용할지 결정하기 위해 sample_submission.csv의 target 값을 사용하고 있습니다.\n",
    "\n",
    "python\n",
    "# /root/DTC/upstageailab-cv-classification-cv_6/test_data_preprocess.py\n",
    "...\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(input_csv) # input_csv = './data/sample_submission.csv'\n",
    "...\n",
    "    for index, row in df.iterrows():\n",
    "        file_name = row['ID']\n",
    "        target = row['target'] # <-- 문제 지점\n",
    "...\n",
    "        # 클래스가 지정된 리스트에 있는 경우 전처리 수행\n",
    "        if target in classes_to_crop: # <-- 문제 지점\n",
    "            processed_image = process_image(image_path)\n",
    "...\n",
    "sample_submission.csv의 target은 실제 정답이 아닌, 제출 형식을 위한 임의의 값입니다. 실제 테스트 환경에서는 정답(target)을 알 수 없으므로 이 로직은 동작할 수 없으며, 현재는 우연히 sample_submission의 값에 따라 전처리가 적용되고 있을 뿐입니다.\n",
    "\n",
    "해결 방안: 모든 테스트 이미지에 동일한 전처리 파이프라인을 적용해야 합니다. 특정 클래스에만 다른 전처리를 적용하고 싶다면, 그 결정은 모델 스스로(예: 1단계 분류 모델) 해야 하며 외부 정보(정답)에 의존해서는 안 됩니다.\n",
    "\n",
    "가장 간단하고 올바른 수정은 조건문을 제거하고 모든 이미지에 일관된 전처리를 적용하거나, 혹은 전처리가 불필요하다고 판단되면 원본을 그대로 사용하는 것입니다. 아래는 모든 이미지에 process_image를 적용하는 수정안입니다.\n",
    "\n",
    "test_data_preprocess.py\n",
    "-39\n",
    "+32\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 크롭할 클래스 리스트\n",
    "    classes_to_crop = [1, 3, 4, 6, 7, 10, 11, 12, 13, 14, 15]\n",
    "    \"\"\"\n",
    "    0,계좌번호\n",
    "    1,지급 신청서\n",
    "    2,자동차 계기판\n",
    "    3,입_퇴원 확인서\n",
    "    4,진단서\n",
    "    5,운전면허증\n",
    "    6,진료비영수증\n",
    "    7,통원_진료 확인서\n",
    "    8,주민등록증\n",
    "    9,여권\n",
    "    10,진료비_납입_확인서\n",
    "    11,약제비_영수증\n",
    "    12,처방전\n",
    "    13,이력서\n",
    "    14,소견서\n",
    "    15,자동차_등록증\n",
    "    16,자동차_번호판\n",
    "    \"\"\"\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    processed_data = []\n",
    "\n",
    "    # 각 행에 대해 반복\n",
    "    for index, row in df.iterrows():\n",
    "    # for index, row in df.iterrows():\n",
    "    #     file_name = row['ID']\n",
    "    #     target = row['target']\n",
    "    #     image_path = os.path.join(input_dir, file_name)\n",
    "    #     print(f\"Processing image: {index} - {file_name}\")\n",
    "\n",
    "    #     # 클래스가 지정된 리스트에 있는 경우 전처리 수행\n",
    "    #     if target in classes_to_crop:\n",
    "    #         processed_image = process_image(image_path)\n",
    "    #         processed_image_path = os.path.join(output_dir, file_name)\n",
    "    #         cv2.imwrite(processed_image_path, processed_image)\n",
    "            \n",
    "    #         # 새로운 CSV 파일에 기록할 데이터 추가\n",
    "    #         processed_data.append({'ID': file_name, 'target': target})\n",
    "    #     else:\n",
    "    #         # 원본 이미지 복사\n",
    "    #         image = load_image(image_path)\n",
    "    #         processed_image_path = os.path.join(output_dir, file_name)\n",
    "    #         cv2.imwrite(processed_image_path, image)\n",
    "            \n",
    "    #         # 처리하지 않은 데이터도 리스트에 추가\n",
    "    #         processed_data.append({'ID': file_name, 'target': target})\n",
    "    \n",
    "    # 모든 이미지에 일관된 전처리 적용\n",
    "    for index, row in df.iterrows():\n",
    "        file_name = row['ID']\n",
    "        target = row['target']\n",
    "        image_path = os.path.join(input_dir, file_name)\n",
    "        print(f\"Processing image: {index} - {file_name}\")\n",
    "\n",
    "        # 클래스가 지정된 리스트에 있는 경우 전처리 수행\n",
    "        if target in classes_to_crop:\n",
    "            processed_image = process_image(image_path)\n",
    "            processed_image_path = os.path.join(output_dir, file_name)\n",
    "            cv2.imwrite(processed_image_path, processed_image)\n",
    "            \n",
    "            # 새로운 CSV 파일에 기록할 데이터 추가\n",
    "            processed_data.append({'ID': file_name, 'target': target})\n",
    "        else:\n",
    "            # 원본 이미지 복사\n",
    "            image = load_image(image_path)\n",
    "            processed_image_path = os.path.join(output_dir, file_name)\n",
    "            cv2.imwrite(processed_image_path, image)\n",
    "            \n",
    "            # 처리하지 않은 데이터도 리스트에 추가\n",
    "            processed_data.append({'ID': file_name, 'target': target})\n",
    "        # 모든 이미지에 대해 전처리 수행\n",
    "        processed_image = process_image(image_path)\n",
    "        processed_image_path = os.path.join(output_dir, file_name)\n",
    "        cv2.imwrite(processed_image_path, processed_image)\n",
    "        \n",
    "        # 새로운 CSV 파일에 기록할 데이터 추가\n",
    "        processed_data.append({'ID': file_name, 'target': target})\n",
    "\n",
    "    print(f\"전처리된 이미지를 {output_dir}에 저장했습니다.\")\n",
    "\n",
    "\n",
    "4. 코드 리팩토링 및 정리\n",
    "main.py 파일에 약간의 중복 코드와 사용되지 않는 함수가 있습니다. 코드를 정리하면 실수를 줄이고 유지보수성을 높일 수 있습니다.\n",
    "\n",
    "중복 DataLoader 정의: train_loader와 val_loader가 두 번 정의되어 있습니다. 첫 번째 정의는 sampler를 사용하고, 두 번째 정의는 shuffle=True를 사용하며 num_workers와 prefetch_factor가 다릅니다. 의도한 설정을 하나만 남기고 나머지는 삭제해야 합니다.\n",
    "사용되지 않는 함수: train_block, just_one_train, n_fold_train 함수들이 정의되어 있지만 실제 if __name__ == \"__main__\": 블록에서는 사용되지 않습니다. main_test.py의 run_fold처럼 모듈화하려는 의도였다면 구조를 완성하거나, 아니라면 혼동을 피하기 위해 삭제하는 것이 좋습니다.\n",
    "main.py\n",
    "-85\n",
    "+0\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg[\"BATCH_SIZE\"],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg[\"BATCH_SIZE\"],\n",
    "    shuffle=True,\n",
    "    num_workers=24,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg[\"BATCH_SIZE\"],\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "def unfreeze(model: nn.Module) -> nn.Parameter:\n",
    "    # 1. 모델의 모든 파라미터를 우선 동결(freeze)합니다.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 2. 특징 추출기(backbone)의 마지막 n개 블록의 동결을 해제(unfreeze)합니다.\n",
    "    # efficientnet_b3는 7개의 블록(0~6)을 가집니다.\n",
    "    for i in range(cfg[\"num_blocks_to_unfreeze\"]):\n",
    "        for param in model.backbone.blocks[-(i+1)].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # 3. 분류기(head)의 동결을 해제합니다.\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # 4. 학습시킬 파라미터만 필터링하여 옵티마이저에 전달합니다.\n",
    "    # requires_grad=True인 파라미터만 업데이트됩니다.\n",
    "    params_to_update = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    \n",
    "    return params_to_update\n",
    "\n",
    "\n",
    "def train_block():\n",
    "    \n",
    "    # load model\n",
    "    model: nn.Module = ModelClass(num_classes=num_classes).to(device)\n",
    "    \n",
    "    if cfg[\"use_unfreeze\"]:\n",
    "        params_to_update = unfreeze(model)\n",
    "    else:\n",
    "        params_to_update = model.parameters()\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=cfg[\"patience\"], delta=cfg[\"delta\"], verbose=True, save_path=save_path, mode='max')\n",
    "\n",
    "    # 손실 함수\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 옵티마이저\n",
    "    optimizer = get_optimizer(cfg_optimizer[\"name\"], params_to_update, cfg_optimizer[\"params\"])\n",
    "\n",
    "    # 스케쥴러\n",
    "    Scheduler = get_scheduler(cfg_scheduler[\"name\"], optimizer, cfg_scheduler['params'])\n",
    "\n",
    "    # amp를 위한 scaler 준비\n",
    "    training_args = {}\n",
    "    if cfg[\"training_mode\"] == 'on_amp':\n",
    "        training_args['scaler'] = GradScaler()\n",
    "\n",
    "    model, valid_max_accuracy = training_loop(\n",
    "        training_fn,\n",
    "        model, train_loader, val_loader, train_dataset, val_dataset, \n",
    "        criterion, optimizer, device, cfg[\"EPOCHS\"], \n",
    "        early_stopping, logger, class_names, Scheduler,\n",
    "        training_args,\n",
    "        )\n",
    "    return model, valid_max_accuracy\n",
    "\n",
    "    \n",
    "def just_one_train():\n",
    "    pass\n",
    "\n",
    "\n",
    "def n_fold_train():\n",
    "    pass\n",
    "\n",
    "\n",
    "def setup_optimizer_params(\n",
    "    model: nn.Module, \n",
    "\n",
    "요약\n",
    "1순위 (성능 향상): Focal Loss를 도입하여 소수 클래스 학습을 강화하세요.\n",
    "2순위 (성능 향상): K-Fold와 앙상블을 결합하여 모델의 일반화 성능과 안정성을 극대화하세요.\n",
    "3순위 (오류 수정): 테스트 데이터 전처리 시 target을 사용하는 치명적인 오류를 수정하세요.\n",
    "4순위 (코드 품질): main.py의 중복 코드를 정리하여 잠재적인 버그를 방지하세요.\n",
    "이 제안들이 F1 Macro 점수를 한 단계 더 끌어올리는 데 도움이 되기를 바랍니다!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06eb42c",
   "metadata": {},
   "source": [
    "추가로 모델 파라미터 더 잘나오게 설정 해봐줄수있어?\n",
    "\n",
    "\n",
    "Gemini\n",
    "안녕하세요! 물론입니다. 현재 main.py에 구현된 차등 학습률(differential learning rate) 설정은 매우 좋은 접근 방식입니다. 여기서 한 걸음 더 나아가, 모델의 성능과 안정성을 높일 수 있는 단계별 파인튜닝(Staged Fine-tuning) 전략과 함께 파라미터 설정 코드를 개선하는 방법을 제안해 드립니다.\n",
    "\n",
    "핵심 개선 아이디어: 단계별 파인튜닝 (Staged Fine-tuning)\n",
    "현재는 모델의 헤드와 백본 일부를 처음부터 함께 학습하고 있습니다. 하지만 사전 학습된 백본의 가중치는 매우 유용한 정보를 담고 있는데, 무작위로 초기화된 헤드(head)의 큰 그래디언트 때문에 이 정보가 초반에 손상될 수 있습니다.\n",
    "\n",
    "단계별 파인튜닝은 이 문제를 해결하는 효과적인 전략입니다.\n",
    "\n",
    "1단계: 헤드 학습 (Feature Extraction)\n",
    "\n",
    "백본(backbone)은 완전히 동결(freeze)하고, 새로운 데이터에 맞게 헤드만 학습시킵니다.\n",
    "이를 통해 백본의 사전 학습된 특징을 그대로 활용하면서, 새로운 클래스를 분류하는 방법만 빠르게 학습합니다.\n",
    "2단계: 전체 미세 조정 (Fine-tuning)\n",
    "\n",
    "헤드가 어느 정도 안정되면, 백본의 동결을 일부 또는 전체 해제합니다.\n",
    "이후, 백본에는 낮은 학습률(learning rate)을, 헤드에는 더 높은 학습률을 적용하는 차등 학습률로 모델 전체를 부드럽게 미세 조정합니다.\n",
    "이 방식을 통해 학습 안정성을 높이고, 모델이 데이터의 특징을 더 잘 학습하여 최종적으로 더 높은 성능을 기대할 수 있습니다.\n",
    "\n",
    "💻 코드 개선 제안\n",
    "위 전략을 적용하기 위해 main.py의 파라미터 설정 함수와 메인 실행 블록을 다음과 같이 개선할 수 있습니다.\n",
    "\n",
    "1. setup_optimizer_params 함수 개선\n",
    "기존 함수를 조금 더 유연하고 안정적으로 만들어, 어떤 모델 구조에도 잘 대응하고 단계별 학습을 제어하기 쉽게 변경합니다.\n",
    "\n",
    "model.neck이 없는 경우에도 오류가 발생하지 않도록 hasattr 체크를 추가합니다.\n",
    "파라미터 그룹을 생성할 때 빈 그룹이 추가되지 않도록 필터링 로직을 강화합니다.\n",
    "2. main.py 메인 실행 블록 수정\n",
    "if __name__ == \"__main__\": 부분을 2단계 학습 로직으로 변경합니다.\n",
    "\n",
    "1단계 학습 루프: num_layers_to_unfreeze=0으로 설정하여 헤드만 학습합니다.\n",
    "2단계 학습 루프: num_layers_to_unfreeze를 설정 파일 값으로 하여 전체를 미세 조정합니다.\n",
    "각 단계에 맞는 옵티마이저와 스케줄러를 새로 생성합니다.\n",
    "아래는 위 내용을 반영한 main.py의 전체 코드 변경 제안입니다.\n",
    "\n",
    "main.py\n",
    "-79\n",
    "+98\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg[\"BATCH_SIZE\"],\n",
    "    shuffle=True,\n",
    "    num_workers=24,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "\n",
    "def setup_optimizer_params(\n",
    "    model: nn.Module, \n",
    "    model_type: str, \n",
    "    num_layers_to_unfreeze: int,\n",
    "    backbone_lr: float,\n",
    "    head_lr: float,\n",
    "    use_differential_lr: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    모델의 동결/해제 상태를 설정하고, 차등 학습률을 적용할 파라미터 그룹을 생성합니다.\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg[\"BATCH_SIZE\"],\n",
    "    shuffle=False, # 검증 데이터는 보통 섞지 않습니다.\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "def setup_optimizer_params(model: nn.Module, model_type: str, num_layers_to_unfreeze: int, backbone_lr: float, head_lr: float, use_differential_lr: bool):\n",
    "    \"\"\"\n",
    "    모델의 동결/해제 상태를 설정하고, 차등 학습률을 적용할 파라미터 그룹을 생성합니다.\n",
    "\n",
    "    :param model: 설정할 PyTorch 모델 객체\n",
    "    :param model_type: 모델의 종류 ('resnet', 'efficientnet', 'swin', 'convnext')\n",
    "    :param backbone_lr: 백본에 적용할 낮은 학습률\n",
    "    :param head_lr: 넥/헤드에 적용할 높은 학습률\n",
    "    :return: 옵티마이저에 전달할 파라미터 그룹 리스트\n",
    "    \"\"\"\n",
    "    \"\"\"    \n",
    "    # 1. 모델의 모든 파라미터를 우선 동결(freeze)합니다.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    backbone = model.backbone\n",
    "    \n",
    "    stages_to_unfreeze = []\n",
    "    if model_type_lower.startswith('resnet'):\n",
    "        all_stages = [backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4]\n",
    "        if num_layers_to_unfreeze > len(all_stages):\n",
    "            num_layers_to_unfreeze = len(all_stages)\n",
    "        stages_to_unfreeze = all_stages[-num_layers_to_unfreeze:]\n",
    "    elif model_type_lower.startswith(('efficientnet', 'swin', 'convnext')):\n",
    "        if model_type_lower.startswith('swin'): all_stages = backbone.layers\n",
    "        elif model_type_lower.startswith('convnext'): all_stages = backbone.stages\n",
    "        else: all_stages = backbone.blocks\n",
    "        num_total_blocks = len(all_stages)\n",
    "        if num_layers_to_unfreeze > num_total_blocks:\n",
    "            num_layers_to_unfreeze = num_total_blocks\n",
    "        stages_to_unfreeze = all_stages[-num_layers_to_unfreeze:]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}.\")\n",
    "    if num_layers_to_unfreeze > 0:\n",
    "        if model_type_lower.startswith('resnet'):\n",
    "            all_stages = [backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4]\n",
    "            if num_layers_to_unfreeze > len(all_stages):\n",
    "                num_layers_to_unfreeze = len(all_stages)\n",
    "            stages_to_unfreeze = all_stages[-num_layers_to_unfreeze:]\n",
    "        elif model_type_lower.startswith(('efficientnet', 'swin', 'convnext')):\n",
    "            if model_type_lower.startswith('swin'): all_stages = backbone.layers\n",
    "            elif model_type_lower.startswith('convnext'): all_stages = backbone.stages\n",
    "            else: all_stages = backbone.blocks\n",
    "            num_total_blocks = len(all_stages)\n",
    "            if num_layers_to_unfreeze > num_total_blocks:\n",
    "                num_layers_to_unfreeze = num_total_blocks\n",
    "            stages_to_unfreeze = all_stages[-num_layers_to_unfreeze:]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {model_type}.\")\n",
    "\n",
    "    for stage in stages_to_unfreeze:\n",
    "        for param in stage.parameters():\n",
    "            \n",
    "    # 4. 차등 학습률을 적용할 파라미터 그룹 생성 혹은 단일 그룹 생성\n",
    "    #    requires_grad=True인 파라미터만 필터링하여 각 그룹에 포함시킵니다.\n",
    "    param_groups = []\n",
    "    if use_differential_lr:\n",
    "        param_groups = [\n",
    "            {\n",
    "                \"params\": filter(lambda p: p.requires_grad, model.backbone.parameters()),\n",
    "                \"lr\": backbone_lr\n",
    "            },\n",
    "            {\n",
    "                \"params\": filter(lambda p: p.requires_grad, model.neck.parameters()),\n",
    "                \"lr\": head_lr\n",
    "            },\n",
    "            {\n",
    "                \"params\": filter(lambda p: p.requires_grad, model.head.parameters()),\n",
    "                \"lr\": head_lr\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"Unfrozen the last {num_layers_to_unfreeze} backbone layers, neck, and head for model type: {model_type}.\")\n",
    "        backbone_params = list(filter(lambda p: p.requires_grad, model.backbone.parameters()))\n",
    "        if backbone_params:\n",
    "            param_groups.append({\"params\": backbone_params, \"lr\": backbone_lr})\n",
    "\n",
    "        if hasattr(model, 'neck'):\n",
    "            neck_params = list(filter(lambda p: p.requires_grad, model.neck.parameters()))\n",
    "            if neck_params:\n",
    "                param_groups.append({\"params\": neck_params, \"lr\": head_lr})\n",
    "\n",
    "        if hasattr(model, 'head'):\n",
    "            head_params = list(filter(lambda p: p.requires_grad, model.head.parameters()))\n",
    "            if head_params:\n",
    "                param_groups.append({\"params\": head_params, \"lr\": head_lr})\n",
    "\n",
    "        print(f\"Unfrozen the last {num_layers_to_unfreeze} backbone layers, neck, and head.\")\n",
    "        print(f\"Applied differential learning rate: backbone_lr={backbone_lr}, head_lr={head_lr}\")\n",
    "    else:\n",
    "        # 단일 학습률 그룹 생성 (학습 가능한 모든 파라미터를 하나의 그룹으로 묶음)\n",
    "        param_groups = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        print(\"Differential learning rate disabled. Using a single LR for all trainable parameters.\")\n",
    "        \n",
    "\n",
    "    return param_groups\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    save_path = f'{output_root}/checkpoint.pth'\n",
    "    \n",
    "    # load model\n",
    "    # 모델 로드\n",
    "    model: nn.Module = ModelClass(num_classes=num_classes).to(device)\n",
    "    \n",
    "    if cfg[\"use_unfreeze\"]:\n",
    "    # 손실 함수\n",
    "    criterion = get_loss(cfg_loss[\"name\"], cfg_loss[\"params\"])\n",
    "\n",
    "    # --- 1단계: 헤드 학습 ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚀 STARTING STAGE 1: HEAD TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 헤드만 학습하도록 파라미터 설정 (백본 동결)\n",
    "    head_params = setup_optimizer_params(\n",
    "        model=model,\n",
    "        model_type=cfg[\"model_type\"], \n",
    "        num_layers_to_unfreeze=0, # 백본은 동결\n",
    "        backbone_lr=0, # 의미 없음\n",
    "        head_lr=cfg[\"head_only_lr\"], # 헤드 학습용 LR\n",
    "        use_differential_lr=False # 모든 학습 파라미터에 단일 LR 적용\n",
    "    )\n",
    "\n",
    "    optimizer_head = get_optimizer(cfg_optimizer[\"name\"], head_params, {\"lr\": cfg[\"head_only_lr\"]})\n",
    "    scheduler_head = get_scheduler(cfg_scheduler[\"name\"], optimizer_head, cfg_scheduler['params'])\n",
    "    \n",
    "    # 1단계 학습 실행\n",
    "    training_loop(\n",
    "        training_fn, model, train_loader, val_loader, train_dataset, val_dataset, \n",
    "        criterion, optimizer_head, device, cfg[\"head_only_epochs\"], \n",
    "        None, logger, class_names, scheduler_head, {}\n",
    "    )\n",
    "\n",
    "    # --- 2단계: 전체 미세 조정 ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚀 STARTING STAGE 2: FULL FINE-TUNING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 백본 일부와 헤드를 함께 학습하도록 파라미터 설정\n",
    "    if cfg[\"use_unfreeze\"]:\n",
    "        params_to_update = setup_optimizer_params(\n",
    "                                model=model,\n",
    "                                model_type=cfg[\"model_type\"], \n",
    "                                num_layers_to_unfreeze=cfg[\"num_blocks_to_unfreeze\"],\n",
    "                                backbone_lr=cfg[\"backbone_lr\"],\n",
    "                                head_lr=cfg_optimizer[\"params\"][\"lr\"],\n",
    "                                use_differential_lr=cfg[\"use_differential_lr\"]\n",
    "                            )\n",
    "            model=model,\n",
    "            model_type=cfg[\"model_type\"], \n",
    "            num_layers_to_unfreeze=cfg[\"num_blocks_to_unfreeze\"],\n",
    "            backbone_lr=cfg[\"backbone_lr\"],\n",
    "            head_lr=cfg_optimizer[\"params\"][\"lr\"],\n",
    "            use_differential_lr=cfg[\"use_differential_lr\"]\n",
    "        )\n",
    "    else:\n",
    "        params_to_update = model.parameters()\n",
    "\n",
    "    # 2단계용 옵티마이저, 스케줄러, EarlyStopping 재설정\n",
    "    save_path = f'{output_root}/{filename}_checkpoint.pth'\n",
    "    early_stopping = EarlyStopping(patience=cfg[\"patience\"], delta=cfg[\"delta\"], verbose=True, save_path=save_path, mode='max')\n",
    "\n",
    "    # 손실 함수\n",
    "    criterion = get_loss(cfg_loss[\"name\"], cfg_loss[\"params\"])\n",
    "\n",
    "    # 옵티마이저\n",
    "    optimizer = get_optimizer(cfg_optimizer[\"name\"], params_to_update, cfg_optimizer[\"params\"])\n",
    "\n",
    "    # 스케쥴러\n",
    "    Scheduler = get_scheduler(cfg_scheduler[\"name\"], optimizer, cfg_scheduler['params'])\n",
    "    \n",
    "    # amp를 위한 scaler 준비\n",
    "    scheduler = get_scheduler(cfg_scheduler[\"name\"], optimizer, cfg_scheduler['params'])\n",
    "    \n",
    "    training_args = {}\n",
    "    if cfg[\"training_mode\"] == 'on_amp':\n",
    "        training_args['scaler'] = GradScaler()\n",
    "\n",
    "    # 2단계 학습 실행\n",
    "    model, valid_max_accuracy = training_loop(\n",
    "        training_fn,\n",
    "        model, train_loader, val_loader, train_dataset, val_dataset, \n",
    "        criterion, optimizer, device, cfg[\"EPOCHS\"], \n",
    "        early_stopping, logger, class_names, Scheduler,\n",
    "        early_stopping, logger, class_names, scheduler,\n",
    "        training_args,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"\\n🎉 Training Finished! Best validation accuracy: {valid_max_accuracy:.4f}\")\n",
    "\n",
    "⚙️ 설정 파일(main_config.yaml) 추가 항목\n",
    "이 새로운 학습 전략을 사용하려면 config/main_config.yaml 파일에 1단계 학습을 제어하기 위한 파라미터를 추가해야 합니다.\n",
    "\n",
    "yaml\n",
    "# config/main_config.yaml\n",
    "\n",
    "# ... 기존 설정 ...\n",
    "\n",
    "# === 단계별 파인튜닝 설정 ===\n",
    "head_only_epochs: 3       # 1단계(헤드만) 학습 에포크 수\n",
    "head_only_lr: 0.001       # 1단계 학습 시 사용할 학습률\n",
    "\n",
    "# === 기존 학습 설정 (2단계용) ===\n",
    "EPOCHS: 15                # 2단계(전체) 학습 에포크 수\n",
    "use_unfreeze: True\n",
    "num_blocks_to_unfreeze: 4\n",
    "use_differential_lr: True\n",
    "backbone_lr: 0.00001\n",
    "\n",
    "optimizer:\n",
    "  name: \"AdamW\"\n",
    "  params:\n",
    "    lr: 0.0001            # 2단계 학습 시 헤드에 적용될 학습률\n",
    "    weight_decay: 0.01\n",
    "\n",
    "# ... 나머지 설정 ...\n",
    "이처럼 학습 과정을 체계적으로 분리하고 각 단계에 맞는 파라미터를 신중하게 설정하면, 모델의 잠재력을 최대한으로 이끌어내어 더 좋은 결과를 얻으실 수 있을 겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed128aa",
   "metadata": {},
   "source": [
    "PyTorch의 OneCycleLR 스케줄러를 단계별 파인튜닝에 적용하는 방법을 알려줘.\n",
    "\n",
    "\n",
    "Gemini\n",
    "안녕하세요! OneCycleLR 스케줄러를 단계별 파인튜닝에 적용하는 것은 모델의 수렴 속도를 높이고 최종 성능을 개선하는 데 매우 효과적인 전략입니다.\n",
    "\n",
    "OneCycleLR의 핵심은 매 배치(step)마다 학습률을 조절한다는 점입니다. 따라서 기존의 에포크(epoch)마다 학습률을 조절하던 방식에서 약간의 코드 수정이 필요합니다.\n",
    "\n",
    "단계별 파인튜닝에 OneCycleLR을 적용하는 전체 과정을 단계별로 안내해 드리겠습니다.\n",
    "\n",
    "1. config/main_config.yaml 파일 수정\n",
    "먼저 설정 파일에서 스케줄러를 OneCycleLR로 변경하고, 관련 하이퍼파라미터를 설정합니다. max_lr과 total_steps는 코드 내에서 동적으로 계산되므로 여기서는 다른 파라미터만 정의합니다.\n",
    "\n",
    "main_config.yaml\n",
    "-6\n",
    "+9\n",
    "BATCH_SIZE: 16\n",
    "EPOCHS: 100\n",
    "EPOCHS: 15 # 2단계(전체) 학습 에포크 수\n",
    "SEED: 42\n",
    "head_only_epochs: 3 # 1단계(헤드만) 학습 에포크 수\n",
    "head_only_lr: 0.001 # 1단계 학습 시 사용할 최대 학습률\n",
    "\n",
    "use_unfreeze: True\n",
    "num_blocks_to_unfreeze: 4 # 모델의 피처 부분을 얼마나 더 학습시킬 것인지\n",
    "model_type: resnet # efficientnet, resnet, convnext, swin\n",
    "  }\n",
    "\n",
    "scheduler:\n",
    "  name: cosine_warm_restart\n",
    "  name: OneCycleLR # 스케줄러 변경\n",
    "  params: {\n",
    "        T_0: 20,\n",
    "        T_mult: 1,\n",
    "        eta_min: 0.0000001,\n",
    "    }\n",
    "    pct_start: 0.3, # 전체 스텝 중 학습률이 올라가는 구간의 비율\n",
    "    div_factor: 25, # 초기 학습률 = max_lr / div_factor\n",
    "    final_div_factor: 1.0e+4, # 마지막 학습률 = 초기 학습률 / final_div_factor\n",
    "  }\n",
    "\n",
    "loss:\n",
    "  name: FocalLoss\n",
    "\n",
    "2. utils/scheduler_factory.py 파일 수정\n",
    "get_scheduler 함수가 OneCycleLR을 생성할 수 있도록 레지스트리에 추가합니다.\n",
    "\n",
    "(해당 파일이 제공되지 않아, 일반적인 구조를 가정하여 작성했습니다. 기존 코드에 맞게 수정해주세요.)\n",
    "\n",
    "scheduler_factory.py\n",
    "-0\n",
    "+11\n",
    "The code change produced by Gemini cannot be automatically applied. You can manually apply the change or ask Gemini to try again.\n",
    "from typing import Iterable\n",
    "\n",
    "from torch.optim import Adam, AdamW, SGD\n",
    "from torch import optim\n",
    "from torch.nn import Module, Parameter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n",
    "\n",
    "\n",
    "OPTIMIZER_REGISTRY: dict[str, optim.Optimizer] = {\n",
    "    \"Adam\": Adam,\n",
    "    \"AdamW\": AdamW,\n",
    "    \"SGD\": SGD,\n",
    "}\n",
    "SCHEDULER_REGISTRY: dict[str, optim.lr_scheduler._LRScheduler] = {\n",
    "    \"cosine_warm_restart\": CosineAnnealingWarmRestarts,\n",
    "    \"OneCycleLR\": OneCycleLR,\n",
    "}\n",
    "\n",
    "def get_optimizer(name: str, params_to_update: Iterable[Parameter], params: dict) -> optim.Optimizer:\n",
    "        raise ValueError(f\"Unknown optimizer: {name}\")\n",
    "    return OPTIMIZER_REGISTRY[name](params_to_update, **params)\n",
    "\n",
    "def get_scheduler(name: str, optimizer: optim.Optimizer, params: dict) -> optim.lr_scheduler._LRScheduler:\n",
    "    if name not in SCHEDULER_REGISTRY:\n",
    "        raise ValueError(f\"Unknown scheduler: {name}\")\n",
    "    # OneCycleLR은 epoch마다 step하는 다른 스케줄러와 달리, 매 step마다 호출되어야 합니다.\n",
    "    # trainer.py의 training_loop 내에서 scheduler.step()이 배치(batch)마다 호출되도록 수정해야 합니다.\n",
    "    return SCHEDULER_REGISTRY[name](optimizer, **params)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from torch import nn\n",
    "\n",
    "3. main.py 파일 수정\n",
    "메인 스크립트에서 각 학습 단계에 맞게 OneCycleLR 스케줄러를 동적으로 생성하도록 수정합니다.\n",
    "\n",
    "1단계: head_only_lr을 max_lr로 사용합니다.\n",
    "2단계: 차등 학습률을 위해 max_lr을 리스트([backbone_lr, head_lr, ...]) 형태로 전달합니다.\n",
    "main.py\n",
    "-3\n",
    "+16\n",
    "        use_differential_lr=False # 모든 학습 파라미터에 단일 LR 적용\n",
    "    )\n",
    "\n",
    "    # 1단계용 OneCycleLR 스케줄러 설정\n",
    "    scheduler_params_stage1 = cfg_scheduler[\"params\"].copy()\n",
    "    scheduler_params_stage1['max_lr'] = cfg[\"head_only_lr\"]\n",
    "    scheduler_params_stage1['total_steps'] = len(train_loader) * cfg[\"head_only_epochs\"]\n",
    "    if scheduler_params_stage1['total_steps'] == 0: scheduler_params_stage1['total_steps'] = 1 # epochs=0일때 에러 방지\n",
    "\n",
    "    optimizer_head = get_optimizer(cfg_optimizer[\"name\"], head_params, {\"lr\": cfg[\"head_only_lr\"]})\n",
    "    scheduler_head = get_scheduler(cfg_scheduler[\"name\"], optimizer_head, cfg_scheduler['params'])\n",
    "    scheduler_head = get_scheduler(cfg_scheduler[\"name\"], optimizer_head, scheduler_params_stage1)\n",
    "    \n",
    "    # 1단계 학습 실행\n",
    "    training_loop(\n",
    "        training_fn, model, train_loader, val_loader, train_dataset, val_dataset, \n",
    "        criterion, optimizer_head, device, cfg[\"head_only_epochs\"], \n",
    "        criterion, optimizer_head, device, cfg[\"head_only_epochs\"],\n",
    "        None, logger, class_names, scheduler_head, {}\n",
    "    )\n",
    "\n",
    "\n",
    "    # 2단계용 옵티마이저, 스케줄러, EarlyStopping 재설정\n",
    "    save_path = f'{output_root}/{filename}_checkpoint.pth'\n",
    "    early_stopping = EarlyStopping(patience=cfg[\"patience\"], delta=cfg[\"delta\"], verbose=True, save_path=save_path, mode='max')\n",
    "    optimizer = get_optimizer(cfg_optimizer[\"name\"], params_to_update, cfg_optimizer[\"params\"])\n",
    "    scheduler = get_scheduler(cfg_scheduler[\"name\"], optimizer, cfg_scheduler['params'])\n",
    "    \n",
    "    # 2단계용 OneCycleLR 스케줄러 설정\n",
    "    scheduler_params_stage2 = cfg_scheduler[\"params\"].copy()\n",
    "    # 차등 학습률을 위해 max_lr을 리스트로 구성\n",
    "    max_lrs = [pg.get('lr', cfg_optimizer['params']['lr']) for pg in optimizer.param_groups]\n",
    "    scheduler_params_stage2['max_lr'] = max_lrs\n",
    "    scheduler_params_stage2['total_steps'] = len(train_loader) * cfg[\"EPOCHS\"]\n",
    "    scheduler = get_scheduler(cfg_scheduler[\"name\"], optimizer, scheduler_params_stage2)\n",
    "\n",
    "    training_args = {}\n",
    "    if cfg[\"training_mode\"] == 'on_amp':\n",
    "        training_args['scaler'] = GradScaler()\n",
    "\n",
    "4. trainer 모듈 수정 (가장 중요)\n",
    "OneCycleLR은 매 배치(step)마다 scheduler.step()을 호출해야 합니다. 현재 training_loop는 에포크(epoch)가 끝날 때 스케줄러를 업데이트할 가능성이 높습니다. training_loop 함수 내부의 학습 루프를 다음과 같이 수정해야 합니다.\n",
    "\n",
    "(파일이 제공되지 않아, trainer/trainer.py에 training_loop가 있다고 가정하고 수정안을 제시합니다.)\n",
    "\n",
    "trainer.py\n",
    "-5\n",
    "+10\n",
    "The code change produced by Gemini cannot be automatically applied. You can manually apply the change or ask Gemini to try again.\n",
    "# 이 파일은 가상의 trainer/trainer.py 입니다. 실제 파일 구조에 맞게 수정하세요.\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def training_loop(\n",
    "    training_fn,\n",
    "            # ... (기존 학습 코드: loss 계산, backpropagation 등)\n",
    "            optimizer.step()\n",
    "\n",
    "            # ⚠️ 중요: OneCycleLR을 위해 스케줄러 업데이트를 배치 루프 안으로 이동\n",
    "            if scheduler is not None:\n",
    "                # OneCycleLR은 배치마다 step을 호출합니다.\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                    scheduler.step()\n",
    "\n",
    "        # --- Epoch 종료 ---\n",
    "        # ⚠️ 기존 코드에서는 여기에 스케줄러 업데이트가 있었을 수 있습니다.\n",
    "        # if scheduler is not None:\n",
    "        #     scheduler.step()\n",
    "\n",
    "        # ... (기존 검증 루프 및 로깅 코드)\n",
    "        # OneCycleLR이 아닌 다른 스케줄러(예: CosineAnnealing)를 위한 로직\n",
    "        if scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "\n",
    "    # ... (나머지 코드)\n",
    "    return model, best_score\n",
    "\n",
    "핵심 변경 사항:\n",
    "\n",
    "scheduler.step() 호출을 에포크 루프의 끝에서 배치(학습) 루프의 끝으로 이동시켰습니다.\n",
    "isinstance를 사용하여 스케줄러가 OneCycleLR일 때만 배치마다 업데이트하고, 다른 스케줄러는 기존처럼 에포크마다 업데이트하도록 하여 호환성을 유지했습니다.\n",
    "이 변경 사항들을 적용하면, 단계별 파인튜닝의 각 단계에서 OneCycleLR 스케줄러가 최적의 학습률을 찾아내어 더 빠르고 안정적으로 모델을 학습시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec7077",
   "metadata": {},
   "source": [
    "그리고 너가 말해줬던 K-Fold 교차 검증과 앙상블의 결합 이것도 한번 적용시켜줬으면 좋겠어\n",
    "\n",
    "\n",
    "Gemini\n",
    "물론입니다! K-Fold 교차 검증과 앙상블을 결합하는 것은 모델의 성능과 안정성을 극대화하는 매우 강력한 전략입니다. 기존의 main_test.py와 inference_ensemble.py를 하나로 합치고, 전체 과정을 체계적으로 관리할 수 있는 새로운 파이프라인을 구성해 드리겠습니다.\n",
    "\n",
    "새로운 워크플로우: run_kfold_ensemble.py\n",
    "이 새로운 접근 방식의 핵심은 하나의 스크립트와 하나의 설정 파일로 전체 과정을 제어하는 것입니다.\n",
    "\n",
    "중앙 설정 파일 (kfold_ensemble_config.yaml): 어떤 모델들을, 몇 개의 Fold로, 어떤 가중치로 앙상블할지 모든 것을 여기서 정의합니다.\n",
    "통합 실행 스크립트 (run_kfold_ensemble.py): 이 스크립트는 두 가지 모드로 동작합니다.\n",
    "train 모드: 설정 파일에 정의된 모든 모델에 대해 K-Fold 교차 검증 학습을 실행하고, 각 Fold의 베스트 모델을 체크포인트로 저장합니다. (예: 3개 모델, 5-Fold -> 15개 체크포인트 생성)\n",
    "inference 모드: train 모드에서 생성된 모든 체크포인트를 자동으로 불러와 가중 평균 앙상블 및 TTA(Test Time Augmentation)를 적용하여 최종 예측 결과를 생성합니다.\n",
    "1. 스케줄러 팩토리 수정\n",
    "먼저, OneCycleLR 스케줄러를 사용할 수 있도록 utils/scheduler_factory.py에 추가합니다.\n",
    "\n",
    "scheduler_factory.py\n",
    "-1\n",
    "+2\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts, OneCycleLR\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "SCHEDULER_REGISTRY: dict[str, type] = {\n",
    "    \"step\": StepLR,\n",
    "    \"multistep\": MultiStepLR,\n",
    "    \"cosine\": CosineAnnealingLR,\n",
    "    \"plateau\": ReduceLROnPlateau,\n",
    "    \"cosine_warm_restart\": CosineAnnealingWarmRestarts,\n",
    "    \"OneCycleLR\": OneCycleLR,\n",
    "}\n",
    "\n",
    "def get_scheduler(name: str, optimizer: optim.Optimizer, params: dict) -> optim.lr_scheduler._LRScheduler:\n",
    "\n",
    "2. 새로운 중앙 설정 파일 생성\n",
    "프로젝트의 루트 config 폴더에 아래 내용으로 kfold_ensemble_config.yaml 파일을 새로 생성합니다. 이 파일이 전체 파이프라인의 \"조종석\" 역할을 합니다.\n",
    "\n",
    "New file: kfold_ensemble_config.yaml\n",
    "+34\n",
    "# Global settings for K-Fold and Ensemble\n",
    "SEED: 42\n",
    "N_SPLITS: 5 # K-Fold의 K 값\n",
    "DATA_PATH: './data'\n",
    "OUTPUT_ROOT: './output'\n",
    "NUM_CLASSES: 17\n",
    "\n",
    "# 앙상블에 사용할 모델 리스트\n",
    "# 각 모델은 고유한 학습 설정(config_path)을 가질 수 있습니다.\n",
    "MODELS:\n",
    "  - # 모델 A: ResNeSt\n",
    "    config_path: \"config/main_config.yaml\" # 이 모델에 대한 학습 설정 파일\n",
    "    weight: 0.4 # 앙상블 가중치\n",
    "\n",
    "  - # 모델 B: EfficientNet (가정)\n",
    "    # main_config.yaml을 복사하여 MODEL 이름만 바꿔서 사용 가능\n",
    "    config_path: \"config/effnet_config.yaml\"\n",
    "    weight: 0.3\n",
    "\n",
    "  - # 모델 C: ConvNeXt (가정)\n",
    "    # main_config.yaml을 복사하여 MODEL 이름만 바꿔서 사용 가능\n",
    "    config_path: \"config/convnext_config.yaml\"\n",
    "    weight: 0.3\n",
    "\n",
    "# 추론 시 설정\n",
    "INFERENCE:\n",
    "  batch_size: 16\n",
    "  use_tta: True\n",
    "  # TTA에 사용할 변환 리스트 (torchvision.transforms 형식)\n",
    "  tta_transforms:\n",
    "    - name: \"RandomHorizontalFlip\"\n",
    "      params: { p: 1.0 }\n",
    "    - name: \"RandomAffine\"\n",
    "      params: { degrees: 10,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b3254",
   "metadata": {},
   "source": [
    "지금 너가 개선해준부분 전부 변경했는데 이러면 어떤 py파일부터 순차적으로 실행해야할까?\n",
    "\n",
    "\n",
    "Gemini\n",
    "네, 그럼요! 지금까지 제안해 드린 모든 개선 사항(단계별 파인튜닝, K-Fold, 앙상블)을 통합하여, 이제 어떤 파일을 어떻게 실행해야 하는지 명확하게 정리해 드리겠습니다.\n",
    "\n",
    "가장 큰 변화는 여러 스크립트를 따로 실행할 필요 없이, 단 하나의 통합 스크립트(run_kfold_ensemble.py)로 전체 학습과 추론 과정을 관리하게 되었다는 점입니다.\n",
    "\n",
    "새로운 실행 순서 요약\n",
    "이제 복잡한 과정은 잊으셔도 됩니다. 실행 순서는 아래와 같이 매우 간단해집니다.\n",
    "\n",
    "(선택 사항) 테스트 데이터 전처리: test_data_preprocess.py를 실행하여 테스트 이미지를 미리 처리합니다.\n",
    "학습 실행: run_kfold_ensemble.py를 train 모드로 실행하여 설정 파일에 정의된 모든 모델을 K-Fold로 학습시킵니다.\n",
    "추론 실행: run_kfold_ensemble.py를 inference 모드로 실행하여 학습된 모든 모델을 자동으로 불러와 앙상블 추론을 수행하고 최종 제출 파일을 생성합니다.\n",
    "⚙️ 1단계: kfold_ensemble_config.yaml 설정하기\n",
    "먼저, 프로젝트의 \"조종석\" 역할을 할 설정 파일을 완성해야 합니다. 이 파일 하나로 어떤 모델을, 몇 Fold로, 어떻게 앙상블할지 모두 제어합니다.\n",
    "\n",
    "config/kfold_ensemble_config.yaml 파일을 아래 내용으로 생성하거나 수정해 주세요.\n",
    "\n",
    "kfold_ensemble_config.yaml\n",
    "-12\n",
    "+18\n",
    "# Global settings for K-Fold and Ensemble\n",
    "SEED: 42\n",
    "N_SPLITS: 5 # K-Fold의 K 값\n",
    "DATA_PATH: './data'\n",
    "DATA_ROOT: './data'\n",
    "OUTPUT_ROOT: './output'\n",
    "NUM_CLASSES: 17\n",
    "\n",
    "# 앙상블에 사용할 모델 리스트\n",
    "# 각 모델은 고유한 학습 설정(config_path)을 가질 수 있습니다.\n",
    "MODELS:\n",
    "  - # 모델 A: ResNeSt\n",
    "  - # --- 모델 A: ResNeSt101e ---\n",
    "    model_name: \"ResNeSt101e\" # 체크포인트 파일명에 사용될 고유 이름\n",
    "    config_path: \"config/main_config.yaml\" # 이 모델에 대한 학습 설정 파일\n",
    "    weight: 0.4 # 앙상블 가중치\n",
    "\n",
    "  - # 모델 B: EfficientNet (가정)\n",
    "    # main_config.yaml을 복사하여 MODEL 이름만 바꿔서 사용 가능\n",
    "    config_path: \"config/effnet_config.yaml\"\n",
    "  - # --- 모델 B: EfficientNetV2 (가정) ---\n",
    "    model_name: \"EfficientNetV2\"\n",
    "    # main_config.yaml을 복사하여 MODEL, model_type 등만 바꿔서 사용\n",
    "    config_path: \"config/effnet_config.yaml\" # 예시 파일명\n",
    "    weight: 0.3\n",
    "\n",
    "  - # 모델 C: ConvNeXt (가정)\n",
    "    # main_config.yaml을 복사하여 MODEL 이름만 바꿔서 사용 가능\n",
    "    config_path: \"config/convnext_config.yaml\"\n",
    "  - # --- 모델 C: ConvNeXt (가정) ---\n",
    "    model_name: \"ConvNeXt\"\n",
    "    # main_config.yaml을 복사하여 MODEL, model_type 등만 바꿔서 사용\n",
    "    config_path: \"config/convnext_config.yaml\" # 예시 파일명\n",
    "    weight: 0.3\n",
    "\n",
    "# 추론 시 설정\n",
    "INFERENCE:\n",
    "  batch_size: 16\n",
    "  use_tta: True\n",
    "  submission_filename: \"submission_ensemble.csv\"\n",
    "  # TTA에 사용할 변환 리스트 (torchvision.transforms 형식)\n",
    "  tta_transforms:\n",
    "    - name: \"RandomHorizontalFlip\"\n",
    "      params: { p: 1.0 }\n",
    "    - name: \"RandomAffine\"\n",
    "      params: { degrees: 10,\n",
    "    - name: \"HorizontalFlip\"\n",
    "      params: { p: 1.0 } # torchvision.transforms.v2\n",
    "    - name: \"RandomRotation\"\n",
    "      params: { degrees: 15 }\n",
    "    - name: \"RandomRotation\"\n",
    "      params: { degrees: -15 }\n",
    "\n",
    "중요: MODELS 리스트에 있는 각 모델의 config_path (예: config/effnet_config.yaml)는 실제로 존재하는 파일이어야 합니다. main_config.yaml을 복사하여 MODEL, model_type 등만 수정해서 여러 개를 만들어두시면 됩니다.\n",
    "\n",
    "🖥️ 2단계: 통합 실행 스크립트 run_kfold_ensemble.py 생성\n",
    "기존의 main.py, main_test.py, inference_ensemble.py의 핵심 기능들을 모두 합친 통합 스크립트입니다. 프로젝트 루트에 아래 내용으로 run_kfold_ensemble.py 파일을 새로 생성해 주세요.\n",
    "\n",
    "New file: run_kfold_ensemble.py\n",
    "+309\n",
    "The code change produced by Gemini cannot be automatically applied. You can manually apply the change or ask Gemini to try again.\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config.config import load_config\n",
    "from utils.utils import *\n",
    "from datasets.transforms import build_unified_transforms, build_tta_transforms\n",
    "\n",
    "from datasets import get_dataset\n",
    "from models import get_model\n",
    "\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from utils.scheduler_factory import get_scheduler\n",
    "from utils.optimizer_factory import get_optimizer\n",
    "from utils.loss_factory import get_loss\n",
    "from trainer import *\n",
    "from trainer.wandb_logger import WandbLogger\n",
    "from utils.predict_tta import get_tta_predictions\n",
    "\n",
    "# --- 유틸리티 함수 (main.py에서 가져옴) ---\n",
    "\n",
    "def setup_optimizer_params(model: nn.Module, model_type: str, num_layers_to_unfreeze: int, backbone_lr: float, head_lr: float, use_differential_lr: bool):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_type_lower = model_type.lower()\n",
    "    backbone = model.backbone\n",
    "    \n",
    "    stages_to_unfreeze = []\n",
    "    if num_layers_to_unfreeze > 0:\n",
    "        if model_type_lower.startswith('resnet'):\n",
    "            all_stages = [backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4]\n",
    "        elif model_type_lower.startswith('swin'): all_stages = backbone.layers\n",
    "        elif model_type_lower.startswith('convnext'): all_stages = backbone.stages\n",
    "        else: all_stages = backbone.blocks # efficientnet\n",
    "        \n",
    "        if num_layers_to_unfreeze > len(all_stages): num_layers_to_unfreeze = len(all_stages)\n",
    "        stages_to_unfreeze = all_stages[-num_layers_to_unfreeze:]\n",
    "\n",
    "    for stage in stages_to_unfreeze:\n",
    "        for param in stage.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(model, 'neck'):\n",
    "        for param in model.neck.parameters(): param.requires_grad = True\n",
    "    if hasattr(model, 'head'):\n",
    "        for param in model.head.parameters(): param.requires_grad = True\n",
    "            \n",
    "    param_groups = []\n",
    "    if use_differential_lr:\n",
    "        backbone_params = list(filter(lambda p: p.requires_grad, model.backbone.parameters()))\n",
    "        if backbone_params: param_groups.append({\"params\": backbone_params, \"lr\": backbone_lr})\n",
    "\n",
    "        if hasattr(model, 'neck'):\n",
    "            neck_params = list(filter(lambda p: p.requires_grad, model.neck.parameters()))\n",
    "            if neck_params: param_groups.append({\"params\": neck_params, \"lr\": head_lr})\n",
    "\n",
    "        if hasattr(model, 'head'):\n",
    "            head_params = list(filter(lambda p: p.requires_grad, model.head.parameters()))\n",
    "            if head_params: param_groups.append({\"params\": head_params, \"lr\": head_lr})\n",
    "    else:\n",
    "        param_groups = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    return param_groups\n",
    "\n",
    "# --- 학습 파이프라인 ---\n",
    "\n",
    "def run_fold_training(\n",
    "    fold: int,\n",
    "    model_config: dict,\n",
    "    global_config: dict,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    device: torch.device\n",
    "):\n",
    "    # --- 설정 불러오기 ---\n",
    "    cfg = load_config(model_config['config_path'])\n",
    "    model_name = model_config['model_name']\n",
    "    \n",
    "    print(f\"\\n===== FOLD {fold+1}/{global_config['N_SPLITS']} | MODEL: {model_name} | CONFIG: {model_config['config_path']} =====\")\n",
    "    \n",
    "    set_seed(global_config['SEED'] + fold) # Fold마다 다른 시드 적용\n",
    "\n",
    "    # --- 데이터셋 및 로더 ---\n",
    "    train_transform, val_transform = build_unified_transforms(cfg[\"transforms\"][\"train\"]), build_unified_transforms(cfg[\"transforms\"][\"val\"])\n",
    "    DatasetClass = get_dataset(cfg['DATASET'])\n",
    "    train_dataset = DatasetClass(df=train_df, data_root=f\"{global_config['DATA_ROOT']}/train/\", transform=train_transform)\n",
    "    val_dataset = DatasetClass(df=val_df, data_root=f\"{global_config['DATA_ROOT']}/train/\", transform=val_transform)\n",
    "    \n",
    "    sampler = setting_sampler(train_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg[\"BATCH_SIZE\"], sampler=sampler, num_workers=os.cpu_count()//2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg[\"BATCH_SIZE\"], shuffle=False, num_workers=os.cpu_count()//2, pin_memory=True)\n",
    "\n",
    "    # --- 모델, 손실함수, 로거 ---\n",
    "    ModelClass = get_model(cfg['MODEL'])\n",
    "    model = ModelClass(num_classes=global_config['NUM_CLASSES']).to(device)\n",
    "    criterion = get_loss(cfg[\"loss\"][\"name\"], cfg[\"loss\"][\"params\"])\n",
    "    \n",
    "    date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    run_name = f\"{model_name}_fold{fold+1}_{date}\"\n",
    "    save_path = f\"{global_config['OUTPUT_ROOT']}/{run_name}_checkpoint.pth\"\n",
    "    \n",
    "    logger = WandbLogger(\n",
    "        project_name=\"DTC-KFold-Ensemble\",\n",
    "        run_name=run_name,\n",
    "        config=cfg,\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "    # --- 1단계: 헤드 학습 ---\n",
    "    print(\"\\n--- STAGE 1: HEAD TRAINING ---\")\n",
    "    if cfg['head_only_epochs'] > 0:\n",
    "        head_params = setup_optimizer_params(model, cfg[\"model_type\"], 0, 0, cfg[\"head_only_lr\"], False)\n",
    "        optimizer_head = get_optimizer(cfg[\"optimizer\"][\"name\"], head_params, {\"lr\": cfg[\"head_only_lr\"]})\n",
    "        \n",
    "        scheduler_params_s1 = cfg[\"scheduler\"][\"params\"].copy()\n",
    "        scheduler_params_s1['max_lr'] = cfg[\"head_only_lr\"]\n",
    "        scheduler_params_s1['total_steps'] = len(train_loader) * cfg[\"head_only_epochs\"]\n",
    "        scheduler_head = get_scheduler(cfg[\"scheduler\"][\"name\"], optimizer_head, scheduler_params_s1)\n",
    "        \n",
    "        training_loop(\n",
    "            TRAINING_REGISTRY[cfg['training_mode']], model, train_loader, val_loader, train_dataset, val_dataset, \n",
    "            criterion, optimizer_head, device, cfg[\"head_only_epochs\"], None, logger, None, scheduler_head, {}\n",
    "        )\n",
    "\n",
    "    # --- 2단계: 전체 미세 조정 ---\n",
    "    print(\"\\n--- STAGE 2: FULL FINE-TUNING ---\")\n",
    "    params_to_update = setup_optimizer_params(\n",
    "        model, cfg[\"model_type\"], cfg[\"num_blocks_to_unfreeze\"],\n",
    "        cfg[\"backbone_lr\"], cfg[\"optimizer\"][\"params\"][\"lr\"], cfg[\"use_differential_lr\"]\n",
    "    )\n",
    "    optimizer = get_optimizer(cfg[\"optimizer\"][\"name\"], params_to_update, cfg[\"optimizer\"][\"params\"])\n",
    "    \n",
    "    scheduler_params_s2 = cfg[\"scheduler\"][\"params\"].copy()\n",
    "    max_lrs = [pg.get('lr', cfg['optimizer']['params']['lr']) for pg in optimizer.param_groups]\n",
    "    scheduler_params_s2['max_lr'] = max_lrs\n",
    "    scheduler_params_s2['total_steps'] = len(train_loader) * cfg[\"EPOCHS\"]\n",
    "    scheduler = get_scheduler(cfg[\"scheduler\"][\"name\"], optimizer, scheduler_params_s2)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=cfg[\"patience\"], delta=cfg[\"delta\"], verbose=True, save_path=save_path, mode='max')\n",
    "    training_args = {'scaler': GradScaler()} if cfg[\"training_mode\"] == 'on_amp' else {}\n",
    "\n",
    "    training_loop(\n",
    "        TRAINING_REGISTRY[cfg['training_mode']], model, train_loader, val_loader, train_dataset, val_dataset, \n",
    "        criterion, optimizer, device, cfg[\"EPOCHS\"], early_stopping, logger, None, scheduler, training_args\n",
    "    )\n",
    "\n",
    "    # 메모리 정리\n",
    "    del model, optimizer, scheduler, train_loader, val_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_pipeline(global_config):\n",
    "    print(\"🚀 STARTING K-FOLD TRAINING PIPELINE 🚀\")\n",
    "    \n",
    "    # 전체 학습 데이터 로드\n",
    "    full_train_df = pd.read_csv(f\"{global_config['DATA_ROOT']}/train_valid_set/train-label-fix-v1.csv\")\n",
    "    \n",
    "    # K-Fold 분할기 설정\n",
    "    skf = StratifiedKFold(n_splits=global_config['N_SPLITS'], shuffle=True, random_state=global_config['SEED'])\n",
    "\n",
    "    for model_cfg in global_config['MODELS']:\n",
    "        # K-Fold 루프 실행\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(full_train_df, full_train_df['target'])):\n",
    "            train_df = full_train_df.iloc[train_idx]\n",
    "            val_df = full_train_df.iloc[val_idx]\n",
    "            \n",
    "            run_fold_training(\n",
    "                fold=fold,\n",
    "                model_config=model_cfg,\n",
    "                global_config=global_config,\n",
    "                train_df=train_df,\n",
    "                val_df=val_df,\n",
    "                device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            )\n",
    "    print(\"\\n🎉 ALL K-FOLD TRAINING FINISHED! 🎉\")\n",
    "\n",
    "\n",
    "# --- 추론 파이프라인 ---\n",
    "\n",
    "def inference_pipeline(global_config):\n",
    "    print(\"🚀 STARTING ENSEMBLE INFERENCE PIPELINE 🚀\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # --- 테스트 데이터셋 준비 ---\n",
    "    # test_data_preprocess.py를 실행했다면 test_modify 폴더 사용\n",
    "    test_dir = f\"{global_config['DATA_ROOT']}/test_modify\"\n",
    "    if not os.path.exists(test_dir):\n",
    "        test_dir = f\"{global_config['DATA_ROOT']}/test\"\n",
    "        print(f\"'{test_dir}' not found. Using original test directory: '{test_dir}'\")\n",
    "\n",
    "    submission_df = pd.read_csv(f\"{global_config['DATA_ROOT']}/sample_submission.csv\")\n",
    "    \n",
    "    # 추론에는 간단한 리사이즈와 텐서 변환만 사용\n",
    "    _, test_transform = build_unified_transforms(load_config(global_config['MODELS'][0]['config_path'])[\"transforms\"][\"val\"])\n",
    "    \n",
    "    DatasetClass = get_dataset('BaseDataset') # 추론 시에는 간단한 데이터셋 사용\n",
    "    test_dataset = DatasetClass(df=submission_df, data_root=test_dir, transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=global_config['INFERENCE']['batch_size'], shuffle=False, num_workers=os.cpu_count()//2)\n",
    "\n",
    "    # --- TTA 변환 준비 ---\n",
    "    tta_transforms = None\n",
    "    if global_config['INFERENCE']['use_tta']:\n",
    "        tta_transforms = build_tta_transforms(global_config['INFERENCE']['tta_transforms'])\n",
    "        print(f\"TTA enabled with {len(tta_transforms)} transforms.\")\n",
    "\n",
    "    # --- 앙상블 추론 ---\n",
    "    all_model_probs = []\n",
    "    model_weights = []\n",
    "\n",
    "    for model_cfg in global_config['MODELS']:\n",
    "        model_name = model_cfg['model_name']\n",
    "        cfg = load_config(model_cfg['config_path'])\n",
    "        \n",
    "        # 해당 모델의 모든 Fold 체크포인트 경로 찾기\n",
    "        fold_checkpoints = sorted(glob.glob(f\"{global_config['OUTPUT_ROOT']}/{model_name}_fold*_checkpoint.pth\"))\n",
    "        if not fold_checkpoints:\n",
    "            print(f\"⚠️ WARNING: No checkpoints found for model '{model_name}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- Inferencing with {model_name} ({len(fold_checkpoints)} folds) ---\")\n",
    "        model_weights.append(model_cfg['weight'])\n",
    "        \n",
    "        model_fold_probs = []\n",
    "        ModelClass = get_model(cfg['MODEL'])\n",
    "\n",
    "        for fold_path in fold_checkpoints:\n",
    "            print(f\"Loading: {os.path.basename(fold_path)}\")\n",
    "            model = ModelClass(num_classes=global_config['NUM_CLASSES']).to(device)\n",
    "            model.load_state_dict(torch.load(fold_path, map_location=device))\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fold_preds = []\n",
    "                for images, _, _ in tqdm(test_loader, desc=f\"Fold {os.path.basename(fold_path)}\", leave=False):\n",
    "                    images = images.to(device)\n",
    "                    \n",
    "                    if tta_transforms:\n",
    "                        # TTA 적용\n",
    "                        avg_probs = get_tta_predictions(model, images, tta_transforms, device)\n",
    "                        fold_preds.append(avg_probs)\n",
    "                    else:\n",
    "                        # TTA 미적용\n",
    "                        outputs = model(images)\n",
    "                        probs = torch.softmax(outputs, dim=1)\n",
    "                        fold_preds.append(probs.cpu())\n",
    "\n",
    "            model_fold_probs.append(torch.cat(fold_preds, dim=0))\n",
    "            \n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # 각 Fold의 예측 확률을 평균\n",
    "        avg_model_probs = torch.stack(model_fold_probs, dim=0).mean(dim=0)\n",
    "        all_model_probs.append(avg_model_probs)\n",
    "\n",
    "    if not all_model_probs:\n",
    "        print(\"❌ ERROR: No models were successfully inferenced. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # --- 가중 앙상블 ---\n",
    "    print(\"\\n--- Performing weighted ensemble ---\")\n",
    "    # 가중치 정규화\n",
    "    total_weight = sum(model_weights)\n",
    "    normalized_weights = [w / total_weight for w in model_weights]\n",
    "\n",
    "    final_probs = torch.zeros_like(all_model_probs[0])\n",
    "    for probs, weight in zip(all_model_probs, normalized_weights):\n",
    "        final_probs += probs * weight\n",
    "\n",
    "    final_predictions = torch.argmax(final_probs, dim=1).numpy()\n",
    "\n",
    "    # --- 제출 파일 생성 ---\n",
    "    submission_df['target'] = final_predictions\n",
    "    submission_path = os.path.join(global_config['OUTPUT_ROOT'], global_config['INFERENCE']['submission_filename'])\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"\\n🎉 Inference finished! Submission file saved to: {submission_path} 🎉\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"K-Fold Ensemble Training and Inference Pipeline\")\n",
    "    parser.add_argument('--mode', type=str, required=True, choices=['train', 'inference'],\n",
    "                        help=\"Set the pipeline to 'train' or 'inference' mode.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 중앙 설정 파일 로드\n",
    "    global_config = load_config(\"config/kfold_ensemble_config.yaml\")\n",
    "    \n",
    "    # 출력 폴더 생성\n",
    "    os.makedirs(global_config['OUTPUT_ROOT'], exist_ok=True)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        train_pipeline(global_config)\n",
    "    elif args.mode == 'inference':\n",
    "        inference_pipeline(global_config)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 3단계: 스크립트 실행하기\n",
    "\n",
    "이제 모든 준비가 끝났습니다. 터미널에서 아래 명령어를 순서대로 실행하시면 됩니다.\n",
    "\n",
    "#### **1. (선택) 테스트 데이터 전처리**\n",
    "만약 `test_data_preprocess.py`를 사용하신다면, 가장 먼저 실행해 주세요.\n",
    "\n",
    "```bash\n",
    "python test_data_preprocess.py\n",
    "이 스크립트는 ./data/test 이미지를 처리하여 ./data/test_modify 폴더에 저장합니다. run_kfold_ensemble.py는 이 폴더를 우선적으로 사용합니다.\n",
    "\n",
    "2. K-Fold 학습 시작\n",
    "아래 명령어로 train 모드를 실행합니다. 스크립트는 kfold_ensemble_config.yaml을 읽어 정의된 모든 모델에 대해 K-Fold 학습을 순차적으로 진행하고, 각 Fold의 베스트 모델을 ./output 폴더에 저장합니다.\n",
    "\n",
    "bash\n",
    "python run_kfold_ensemble.py --mode train\n",
    "3. 앙상블 추론 및 제출 파일 생성\n",
    "학습이 모두 끝나면, 아래 명령어로 inference 모드를 실행합니다. 스크립트가 ./output 폴더에서 학습된 모든 체크포인트를 자동으로 찾아 TTA와 가중 앙상블을 적용한 후, 최종 제출 파일(submission_ensemble.csv)을 생성합니다.\n",
    "\n",
    "bash\n",
    "python run_kfold_ensemble.py --mode inference\n",
    "이제 이 새로운 파이프라인을 통해 훨씬 더 체계적이고 재현 가능하게 실험을 관리하실 수 있습니다. 성공적인 결과 있기를 바랍니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4d200",
   "metadata": {},
   "source": [
    "너가 위에 말했던것들 전부 다시 반영해서 코드 만들어줘\n",
    "\n",
    "\n",
    "Gemini\n",
    "네, 알겠습니다! 지금까지 논의했던 모든 개선 사항—단계별 파인튜닝, K-Fold 교차 검증, TTA, 그리고 가중 앙상블—을 모두 통합한 최종 코드를 다시 만들어 드리겠습니다.\n",
    "\n",
    "여러 스크립트를 복잡하게 실행할 필요 없이, 단 하나의 통합 스크립트(run_kfold_ensemble.py)와 설정 파일(kfold_ensemble_config.yaml)로 전체 파이프라인을 제어하도록 구성했습니다.\n",
    "\n",
    "새로운 워크플로우 요약\n",
    "중앙 설정 (kfold_ensemble_config.yaml): 앙상블에 사용할 모든 모델, K-Fold 설정, TTA, 가중치 등을 이 파일 하나로 관리합니다.\n",
    "통합 실행 (run_kfold_ensemble.py): 이 스크립트를 --mode 옵션과 함께 실행하여 학습과 추론을 모두 수행합니다.\n",
    "--mode train: 설정 파일에 정의된 모든 모델에 대해 K-Fold 학습을 실행하고, 각 Fold의 베스트 모델을 체크포인트로 저장합니다.\n",
    "--mode inference: 저장된 모든 체크포인트를 자동으로 불러와 TTA와 가중 앙상블을 적용하여 최종 제출 파일을 생성합니다.\n",
    "1. 중앙 설정 파일 (kfold_ensemble_config.yaml)\n",
    "먼저, 프로젝트의 \"조종석\" 역할을 할 설정 파일입니다. 제공해주신 내용을 기반으로 최종 버전을 다시 확인합니다. 이 파일은 config/ 폴더에 위치해야 합니다.\n",
    "\n",
    "yaml\n",
    "# /root/DTC/upstageailab-cv-classification-cv_6/config/kfold_ensemble_config.yaml\n",
    "\n",
    "# Global settings for K-Fold and Ensemble\n",
    "SEED: 42\n",
    "N_SPLITS: 5 # K-Fold의 K 값\n",
    "DATA_ROOT: './data'\n",
    "OUTPUT_ROOT: './output'\n",
    "NUM_CLASSES: 17\n",
    "\n",
    "# 앙상블에 사용할 모델 리스트\n",
    "# 각 모델은 고유한 학습 설정(config_path)을 가질 수 있습니다.\n",
    "MODELS:\n",
    "  - # --- 모델 A: ResNeSt101e ---\n",
    "    model_name: \"ResNeSt101e\" # 체크포인트 파일명에 사용될 고유 이름\n",
    "    config_path: \"config/main_config.yaml\" # 이 모델에 대한 학습 설정 파일\n",
    "    weight: 0.4 # 앙상블 가중치\n",
    "\n",
    "  - # --- 모델 B: EfficientNetV2 (가정) ---\n",
    "    model_name: \"EfficientNetV2\"\n",
    "    # main_config.yaml을 복사하여 MODEL, model_type 등만 바꿔서 사용\n",
    "    config_path: \"config/effnet_config.yaml\" # 예시 파일명\n",
    "    weight: 0.3\n",
    "\n",
    "  - # --- 모델 C: ConvNeXt (가정) ---\n",
    "    model_name: \"ConvNeXt\"\n",
    "    # main_config.yaml을 복사하여 MODEL, model_type 등만 바꿔서 사용\n",
    "    config_path: \"config/convnext_config.yaml\" # 예시 파일명\n",
    "    weight: 0.3\n",
    "\n",
    "# 추론 시 설정\n",
    "INFERENCE:\n",
    "  batch_size: 16\n",
    "  use_tta: True\n",
    "  submission_filename: \"submission_ensemble.csv\"\n",
    "  # TTA에 사용할 변환 리스트 (torchvision.transforms.v2 형식)\n",
    "  tta_transforms:\n",
    "    - name: \"HorizontalFlip\"\n",
    "      params: { p: 1.0 }\n",
    "    - name: \"RandomRotation\"\n",
    "      params: { degrees: 15 }\n",
    "    - name: \"RandomRotation\"\n",
    "      params: { degrees: -15 }\n",
    "중요: MODELS 리스트에 있는 각 모델의 config_path (예: config/effnet_config.yaml)는 실제로 존재하는 파일이어야 합니다. main_config.yaml을 복사하여 MODEL, model_type 등만 수정해서 여러 개를 만들어두시면 됩니다.\n",
    "\n",
    "2. 통합 실행 스크립트 (run_kfold_ensemble.py)\n",
    "기존의 main.py, main_test.py, inference_ensemble.py의 핵심 기능들을 모두 합친 통합 스크립트입니다. 프로젝트 루트 폴더에 아래 내용으로 run_kfold_ensemble.py 파일을 새로 생성해 주세요.\n",
    "\n",
    "New file: run_kfold_ensemble.py\n",
    "+309\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config.config import load_config\n",
    "from utils.utils import *\n",
    "from datasets.transforms import build_unified_transforms, build_tta_transforms\n",
    "\n",
    "from datasets import get_dataset\n",
    "from models import get_model\n",
    "\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from utils.scheduler_factory import get_scheduler\n",
    "from utils.optimizer_factory import get_optimizer\n",
    "from utils.loss_factory import get_loss\n",
    "from trainer import *\n",
    "from trainer.wandb_logger import WandbLogger\n",
    "from utils.predict_tta import get_tta_predictions\n",
    "\n",
    "# --- 유틸리티 함수 (main.py에서 가져옴) ---\n",
    "\n",
    "def setup_optimizer_params(model: nn.Module, model_type: str, num_layers_to_unfreeze: int, backbone_lr: float, head_lr: float, use_differential_lr: bool):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_type_lower = model_type.lower()\n",
    "    backbone = model.backbone\n",
    "    \n",
    "    stages_to_unfreeze = []\n",
    "    if num_layers_to_unfreeze > 0:\n",
    "        if model_type_lower.startswith('resnet'):\n",
    "            all_stages = [backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4]\n",
    "        elif model_type_lower.startswith('swin'): all_stages = backbone.layers\n",
    "        elif model_type_lower.startswith('convnext'): all_stages = backbone.stages\n",
    "        else: all_stages = backbone.blocks # efficientnet\n",
    "        \n",
    "        if num_layers_to_unfreeze > len(all_stages): num_layers_to_unfreeze = len(all_stages)\n",
    "        stages_to_unfreeze = all_stages[-num_layers_to_unfreeze:]\n",
    "\n",
    "    for stage in stages_to_unfreeze:\n",
    "        for param in stage.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(model, 'neck'):\n",
    "        for param in model.neck.parameters(): param.requires_grad = True\n",
    "    if hasattr(model, 'head'):\n",
    "        for param in model.head.parameters(): param.requires_grad = True\n",
    "            \n",
    "    param_groups = []\n",
    "    if use_differential_lr:\n",
    "        backbone_params = list(filter(lambda p: p.requires_grad, model.backbone.parameters()))\n",
    "        if backbone_params: param_groups.append({\"params\": backbone_params, \"lr\": backbone_lr})\n",
    "\n",
    "        if hasattr(model, 'neck'):\n",
    "            neck_params = list(filter(lambda p: p.requires_grad, model.neck.parameters()))\n",
    "            if neck_params: param_groups.append({\"params\": neck_params, \"lr\": head_lr})\n",
    "\n",
    "        if hasattr(model, 'head'):\n",
    "            head_params = list(filter(lambda p: p.requires_grad, model.head.parameters()))\n",
    "            if head_params: param_groups.append({\"params\": head_params, \"lr\": head_lr})\n",
    "    else:\n",
    "        param_groups = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    return param_groups\n",
    "\n",
    "# --- 학습 파이프라인 ---\n",
    "\n",
    "def run_fold_training(\n",
    "    fold: int,\n",
    "    model_config: dict,\n",
    "    global_config: dict,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    device: torch.device\n",
    "):\n",
    "    # --- 설정 불러오기 ---\n",
    "    cfg = load_config(model_config['config_path'])\n",
    "    model_name = model_config['model_name']\n",
    "    \n",
    "    print(f\"\\n===== FOLD {fold+1}/{global_config['N_SPLITS']} | MODEL: {model_name} | CONFIG: {model_config['config_path']} =====\")\n",
    "    \n",
    "    set_seed(global_config['SEED'] + fold) # Fold마다 다른 시드 적용\n",
    "\n",
    "    # --- 데이터셋 및 로더 ---\n",
    "    train_transform, val_transform = build_unified_transforms(cfg[\"transforms\"][\"train\"]), build_unified_transforms(cfg[\"transforms\"][\"val\"])\n",
    "    DatasetClass = get_dataset(cfg['DATASET'])\n",
    "    train_dataset = DatasetClass(df=train_df, data_root=f\"{global_config['DATA_ROOT']}/train/\", transform=train_transform)\n",
    "    val_dataset = DatasetClass(df=val_df, data_root=f\"{global_config['DATA_ROOT']}/train/\", transform=val_transform)\n",
    "    \n",
    "    sampler = setting_sampler(train_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg[\"BATCH_SIZE\"], sampler=sampler, num_workers=os.cpu_count()//2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg[\"BATCH_SIZE\"], shuffle=False, num_workers=os.cpu_count()//2, pin_memory=True)\n",
    "\n",
    "    # --- 모델, 손실함수, 로거 ---\n",
    "    ModelClass = get_model(cfg['MODEL'])\n",
    "    model = ModelClass(num_classes=global_config['NUM_CLASSES']).to(device)\n",
    "    criterion = get_loss(cfg[\"loss\"][\"name\"], cfg[\"loss\"][\"params\"])\n",
    "    \n",
    "    date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    run_name = f\"{model_name}_fold{fold+1}_{date}\"\n",
    "    save_path = f\"{global_config['OUTPUT_ROOT']}/{run_name}_checkpoint.pth\"\n",
    "    \n",
    "    logger = WandbLogger(\n",
    "        project_name=\"DTC-KFold-Ensemble\",\n",
    "        run_name=run_name,\n",
    "        config=cfg,\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "    # --- 1단계: 헤드 학습 ---\n",
    "    print(\"\\n--- STAGE 1: HEAD TRAINING ---\")\n",
    "    if cfg['head_only_epochs'] > 0:\n",
    "        head_params = setup_optimizer_params(model, cfg[\"model_type\"], 0, 0, cfg[\"head_only_lr\"], False)\n",
    "        optimizer_head = get_optimizer(cfg[\"optimizer\"][\"name\"], head_params, {\"lr\": cfg[\"head_only_lr\"]})\n",
    "        \n",
    "        scheduler_params_s1 = cfg[\"scheduler\"][\"params\"].copy()\n",
    "        scheduler_params_s1['max_lr'] = cfg[\"head_only_lr\"]\n",
    "        scheduler_params_s1['total_steps'] = len(train_loader) * cfg[\"head_only_epochs\"]\n",
    "        if scheduler_params_s1['total_steps'] == 0: scheduler_params_s1['total_steps'] = 1\n",
    "        scheduler_head = get_scheduler(cfg[\"scheduler\"][\"name\"], optimizer_head, scheduler_params_s1)\n",
    "        \n",
    "        training_loop(\n",
    "            TRAINING_REGISTRY[cfg['training_mode']], model, train_loader, val_loader, train_dataset, val_dataset, \n",
    "            criterion, optimizer_head, device, cfg[\"head_only_epochs\"], None, logger, None, scheduler_head, {}\n",
    "        )\n",
    "\n",
    "    # --- 2단계: 전체 미세 조정 ---\n",
    "    print(\"\\n--- STAGE 2: FULL FINE-TUNING ---\")\n",
    "    params_to_update = setup_optimizer_params(\n",
    "        model, cfg[\"model_type\"], cfg[\"num_blocks_to_unfreeze\"],\n",
    "        cfg[\"backbone_lr\"], cfg[\"optimizer\"][\"params\"][\"lr\"], cfg[\"use_differential_lr\"]\n",
    "    )\n",
    "    optimizer = get_optimizer(cfg[\"optimizer\"][\"name\"], params_to_update, cfg[\"optimizer\"][\"params\"])\n",
    "    \n",
    "    scheduler_params_s2 = cfg[\"scheduler\"][\"params\"].copy()\n",
    "    max_lrs = [pg.get('lr', cfg['optimizer']['params']['lr']) for pg in optimizer.param_groups]\n",
    "    scheduler_params_s2['max_lr'] = max_lrs\n",
    "    scheduler_params_s2['total_steps'] = len(train_loader) * cfg[\"EPOCHS\"]\n",
    "    scheduler = get_scheduler(cfg[\"scheduler\"][\"name\"], optimizer, scheduler_params_s2)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=cfg[\"patience\"], delta=cfg[\"delta\"], verbose=True, save_path=save_path, mode='max')\n",
    "    training_args = {'scaler': GradScaler()} if cfg[\"training_mode\"] == 'on_amp' else {}\n",
    "\n",
    "    training_loop(\n",
    "        TRAINING_REGISTRY[cfg['training_mode']], model, train_loader, val_loader, train_dataset, val_dataset, \n",
    "        criterion, optimizer, device, cfg[\"EPOCHS\"], early_stopping, logger, None, scheduler, training_args\n",
    "    )\n",
    "\n",
    "    # 메모리 정리\n",
    "    del model, optimizer, scheduler, train_loader, val_loader, logger, early_stopping\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_pipeline(global_config):\n",
    "    print(\"🚀 STARTING K-FOLD TRAINING PIPELINE 🚀\")\n",
    "    \n",
    "    # 전체 학습 데이터 로드\n",
    "    full_train_df = pd.read_csv(f\"{global_config['DATA_ROOT']}/train_valid_set/train-label-fix-v1.csv\")\n",
    "    \n",
    "    # K-Fold 분할기 설정\n",
    "    skf = StratifiedKFold(n_splits=global_config['N_SPLITS'], shuffle=True, random_state=global_config['SEED'])\n",
    "\n",
    "    for model_cfg in global_config['MODELS']:\n",
    "        # K-Fold 루프 실행\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(full_train_df, full_train_df['target'])):\n",
    "            train_df = full_train_df.iloc[train_idx]\n",
    "            val_df = full_train_df.iloc[val_idx]\n",
    "            \n",
    "            run_fold_training(\n",
    "                fold=fold,\n",
    "                model_config=model_cfg,\n",
    "                global_config=global_config,\n",
    "                train_df=train_df,\n",
    "                val_df=val_df,\n",
    "                device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            )\n",
    "    print(\"\\n🎉 ALL K-FOLD TRAINING FINISHED! 🎉\")\n",
    "\n",
    "\n",
    "# --- 추론 파이프라인 ---\n",
    "\n",
    "def inference_pipeline(global_config):\n",
    "    print(\"🚀 STARTING ENSEMBLE INFERENCE PIPELINE 🚀\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # --- 테스트 데이터셋 준비 ---\n",
    "    # test_data_preprocess.py를 실행했다면 test_modify 폴더 사용\n",
    "    test_dir = f\"{global_config['DATA_ROOT']}/test_modify\"\n",
    "    if not os.path.exists(test_dir) or not os.listdir(test_dir):\n",
    "        test_dir = f\"{global_config['DATA_ROOT']}/test\"\n",
    "        print(f\"'{global_config['DATA_ROOT']}/test_modify' not found or empty. Using original test directory: '{test_dir}'\")\n",
    "\n",
    "    submission_df = pd.read_csv(f\"{global_config['DATA_ROOT']}/sample_submission.csv\")\n",
    "    \n",
    "    # 추론에는 간단한 리사이즈와 텐서 변환만 사용 (첫 번째 모델의 val transform 기준)\n",
    "    _, test_transform = build_unified_transforms(load_config(global_config['MODELS'][0]['config_path'])[\"transforms\"][\"val\"])\n",
    "    \n",
    "    DatasetClass = get_dataset('BaseDataset') # 추론 시에는 간단한 데이터셋 사용\n",
    "    test_dataset = DatasetClass(df=submission_df, data_root=test_dir, transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=global_config['INFERENCE']['batch_size'], shuffle=False, num_workers=os.cpu_count()//2)\n",
    "\n",
    "    # --- TTA 변환 준비 ---\n",
    "    tta_transforms = None\n",
    "    if global_config['INFERENCE']['use_tta']:\n",
    "        tta_transforms = build_tta_transforms(global_config['INFERENCE']['tta_transforms'])\n",
    "        print(f\"TTA enabled with {len(tta_transforms)} transforms.\")\n",
    "\n",
    "    # --- 앙상블 추론 ---\n",
    "    all_model_probs = []\n",
    "    model_weights = []\n",
    "\n",
    "    for model_cfg in global_config['MODELS']:\n",
    "        model_name = model_cfg['model_name']\n",
    "        cfg = load_config(model_cfg['config_path'])\n",
    "        \n",
    "        # 해당 모델의 모든 Fold 체크포인트 경로 찾기\n",
    "        fold_checkpoints = sorted(glob.glob(f\"{global_config['OUTPUT_ROOT']}/{model_name}_fold*_checkpoint.pth\"))\n",
    "        if not fold_checkpoints:\n",
    "            print(f\"⚠️ WARNING: No checkpoints found for model '{model_name}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- Inferencing with {model_name} ({len(fold_checkpoints)} folds) ---\")\n",
    "        model_weights.append(model_cfg['weight'])\n",
    "        \n",
    "        model_fold_probs = []\n",
    "        ModelClass = get_model(cfg['MODEL'])\n",
    "\n",
    "        for fold_path in fold_checkpoints:\n",
    "            print(f\"Loading: {os.path.basename(fold_path)}\")\n",
    "            model = ModelClass(num_classes=global_config['NUM_CLASSES']).to(device)\n",
    "            model.load_state_dict(torch.load(fold_path, map_location=device))\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fold_preds = []\n",
    "                for images, _, _ in tqdm(test_loader, desc=f\"Fold {os.path.basename(fold_path).split('_')[1]}\", leave=False):\n",
    "                    images = images.to(device)\n",
    "                    \n",
    "                    if tta_transforms:\n",
    "                        # TTA 적용\n",
    "                        avg_probs = get_tta_predictions(model, images, tta_transforms, device)\n",
    "                        fold_preds.append(avg_probs)\n",
    "                    else:\n",
    "                        # TTA 미적용\n",
    "                        outputs = model(images)\n",
    "                        probs = torch.softmax(outputs, dim=1)\n",
    "                        fold_preds.append(probs.cpu())\n",
    "\n",
    "            model_fold_probs.append(torch.cat(fold_preds, dim=0))\n",
    "            \n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # 각 Fold의 예측 확률을 평균\n",
    "        avg_model_probs = torch.stack(model_fold_probs, dim=0).mean(dim=0)\n",
    "        all_model_probs.append(avg_model_probs)\n",
    "\n",
    "    if not all_model_probs:\n",
    "        print(\"❌ ERROR: No models were successfully inferenced. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # --- 가중 앙상블 ---\n",
    "    print(\"\\n--- Performing weighted ensemble ---\")\n",
    "    # 가중치 정규화\n",
    "    total_weight = sum(model_weights)\n",
    "    normalized_weights = [w / total_weight for w in model_weights]\n",
    "\n",
    "    final_probs = torch.zeros_like(all_model_probs[0])\n",
    "    for probs, weight in zip(all_model_probs, normalized_weights):\n",
    "        final_probs += probs * weight\n",
    "\n",
    "    final_predictions = torch.argmax(final_probs, dim=1).numpy()\n",
    "\n",
    "    # --- 제출 파일 생성 ---\n",
    "    submission_df['target'] = final_predictions\n",
    "    submission_path = os.path.join(global_config['OUTPUT_ROOT'], global_config['INFERENCE']['submission_filename'])\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"\\n🎉 Inference finished! Submission file saved to: {submission_path} 🎉\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"K-Fold Ensemble Training and Inference Pipeline\")\n",
    "    parser.add_argument('--mode', type=str, required=True, choices=['train', 'inference'],\n",
    "                        help=\"Set the pipeline to 'train' or 'inference' mode.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 중앙 설정 파일 로드\n",
    "    global_config = load_config(\"config/kfold_ensemble_config.yaml\")\n",
    "    \n",
    "    # 출력 폴더 생성\n",
    "    os.makedirs(global_config['OUTPUT_ROOT'], exist_ok=True)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        train_pipeline(global_config)\n",
    "    elif args.mode == 'inference':\n",
    "        inference_pipeline(global_config)\n",
    "\n",
    "🚀 3. 실행 방법\n",
    "이제 모든 준비가 끝났습니다. 터미널에서 아래 명령어를 순서대로 실행하시면 됩니다.\n",
    "\n",
    "1. (선택) 테스트 데이터 전처리\n",
    "만약 test_data_preprocess.py를 사용하신다면, 가장 먼저 실행해 주세요. 이 스크립트는 ./data/test 이미지를 처리하여 ./data/test_modify 폴더에 저장합니다. 추론 스크립트는 이 폴더를 우선적으로 사용합니다.\n",
    "\n",
    "bash\n",
    "python test_data_preprocess.py\n",
    "2. K-Fold 학습 시작\n",
    "아래 명령어로 train 모드를 실행합니다. 스크립트는 kfold_ensemble_config.yaml을 읽어 정의된 모든 모델에 대해 K-Fold 학습을 순차적으로 진행하고, 각 Fold의 베스트 모델을 ./output 폴더에 저장합니다.\n",
    "\n",
    "bash\n",
    "python run_kfold_ensemble.py --mode train\n",
    "3. 앙상블 추론 및 제출 파일 생성\n",
    "학습이 모두 끝나면, 아래 명령어로 inference 모드를 실행합니다. 스크립트가 ./output 폴더에서 학습된 모든 체크포인트를 자동으로 찾아 TTA와 가중 앙상블을 적용한 후, 최종 제출 파일(submission_ensemble.csv)을 생성합니다.\n",
    "\n",
    "bash\n",
    "python run_kfold_ensemble.py --mode inference\n",
    "이 새로운 파이프라인을 통해 훨씬 더 체계적이고 재현 가능하게 실험을 관리하실 수 있습니다. 성공적인 결과 있기를 바랍니다!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
