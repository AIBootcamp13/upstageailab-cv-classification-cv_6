import numpy as np
from tqdm import tqdm
import torch
from sklearn.metrics import f1_score
from torch.cuda.amp import autocast

from datasets.shack import cutmix_data, mixup_criterion


def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):
  model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
  train_loss = 0.0
  train_accuracy = 0
  all_preds_list = []
  all_labels_list = []
  use_cutmix = np.random.rand() < 0.5

  tbar = tqdm(dataloader)
  for images, labels, _ in tbar:
      images = images.to(device)
      labels = labels.to(device)
      
    # ìˆœì „íŒŒ
    # 50% í™•ë¥ ë¡œ CutMix ì ìš©
      if use_cutmix:
          mixed_inputs, targets_a, targets_b, lam = cutmix_data(images, labels, alpha=1.0)
          outputs = model(mixed_inputs, labels)
          loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
      else:
          outputs = model(images, labels)
          loss = criterion(outputs, labels)

      # ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      # ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
      train_loss += loss.item()
      # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
      _, predicted = torch.max(outputs, 1)
      
      # CutMixê°€ ì ìš©ë˜ì—ˆì„ ë•Œì™€ ì•„ë‹ ë•Œë¥¼ êµ¬ë¶„í•˜ì—¬ ì •í™•ë„ ê³„ì‚°
      if use_cutmix: # CutMixê°€ ì ìš©ëœ ê²½ìš°
          correct_predictions = lam * (predicted == targets_a.data).sum().item() + (1 - lam) * (predicted == targets_b.data).sum().item()
          train_accuracy += correct_predictions
      else: # ì¼ë°˜ì ì¸ ê²½ìš°
          train_accuracy += (predicted == labels).sum().item()

      all_preds_list.extend(predicted.detach().cpu().numpy())
      all_labels_list.extend(labels.detach().cpu().numpy())

      # tqdmì˜ ì§„í–‰ë°”ì— í‘œì‹œë  ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •
      tbar.set_description(f"Epoch [{epoch}/{num_epochs}], Train Loss: {loss.item():.4f}")

  # ì—í­ë³„ í•™ìŠµ ê²°ê³¼ ì¶œë ¥
  train_loss = train_loss / len(dataloader)
  train_accuracy = train_accuracy / len(train_dataset)
  train_f1 = f1_score(all_labels_list, all_preds_list, average='macro')


  ret = {
      "train_loss": train_loss,
      "train_accuracy": train_accuracy,
      "train_f1": train_f1,
    }

  return model, ret


def training_use_amp(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs, scaler):
  model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
  train_loss = 0.0
  train_accuracy = 0
  all_preds_list = []
  all_labels_list = []
  use_cutmix = np.random.rand() < 0.5

  tbar = tqdm(dataloader)
  for images, labels, _ in tbar:
      images = images.to(device)
      labels = labels.to(device)

      optimizer.zero_grad()
      
      # ğŸ”¥ autocastë¡œ float16 ì‚¬ìš©
      with autocast():
          # 50% í™•ë¥ ë¡œ CutMix ì ìš©
          if use_cutmix:
              mixed_inputs, targets_a, targets_b, lam = cutmix_data(images, labels, alpha=1.0)
              outputs = model(mixed_inputs, labels)
              loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
          else:
              outputs = model(images, labels)
              loss = criterion(outputs, labels)
      
      
      # âš™ï¸ AMP-aware backward + step
      scaler.scale(loss).backward()
      scaler.step(optimizer)
      scaler.update()


      # ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
      train_loss += loss.item()
      # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
      _, predicted = torch.max(outputs, 1)

      # CutMixê°€ ì ìš©ë˜ì—ˆì„ ë•Œì™€ ì•„ë‹ ë•Œë¥¼ êµ¬ë¶„í•˜ì—¬ ì •í™•ë„ ê³„ì‚°
      if use_cutmix: # CutMixê°€ ì ìš©ëœ ê²½ìš°
          correct_predictions = lam * (predicted == targets_a.data).sum().item() + (1 - lam) * (predicted == targets_b.data).sum().item()
          train_accuracy += correct_predictions
      else: # ì¼ë°˜ì ì¸ ê²½ìš°
          train_accuracy += (predicted == labels).sum().item()

      all_preds_list.extend(predicted.detach().cpu().numpy())
      all_labels_list.extend(labels.detach().cpu().numpy())

      # tqdmì˜ ì§„í–‰ë°”ì— í‘œì‹œë  ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •
      tbar.set_description(f"Epoch [{epoch}/{num_epochs}], Train Loss: {loss.item():.4f}")

  # ì—í­ë³„ í•™ìŠµ ê²°ê³¼ ì¶œë ¥
  train_loss = train_loss / len(dataloader)
  train_accuracy = train_accuracy / len(train_dataset)
  train_f1 = f1_score(all_labels_list, all_preds_list, average='macro')
  
  
  ret = {
      "train_loss": train_loss,
      "train_accuracy": train_accuracy,
      "train_f1": train_f1,
    }

  return model, ret