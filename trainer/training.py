import numpy as np
from tqdm import tqdm
import torch
from sklearn.metrics import f1_score
from torch.cuda.amp import autocast

from datasets.shack import cutmix_data, mixup_criterion


def training(model, dataloader, train_dataset, optimizer, device, epoch, num_epochs, scaler, criterions, miner, triplet_loss_weight):
    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
    train_loss = 0.0
    train_accuracy = 0
    all_preds_list = []
    all_labels_list = []
    use_cutmix = np.random.rand() < 0.5
    
    # ì†ì‹¤ í•¨ìˆ˜ë“¤ì„ ë°›ì•„ì˜µë‹ˆë‹¤.
    criterion_focal = criterions['focal']
    criterion_triplet = criterions['triplet']

    tbar = tqdm(dataloader)
    for images, labels, _ in tbar:
        images = images.to(device)
        labels = labels.to(device)
        
    # ìˆœì „íŒŒ
    # 50% í™•ë¥ ë¡œ CutMix ì ìš©
        if use_cutmix:
            mixed_inputs, targets_a, targets_b, lam = cutmix_data(images, labels, alpha=1.0)
            embeddings, outputs = model(mixed_inputs, labels)
        else:
            embeddings, outputs = model(images, labels)

        # Focal Loss ê³„ì‚°
        if use_cutmix:
            loss_focal = mixup_criterion(criterion_focal, outputs, targets_a, targets_b, lam)
        else:
            loss_focal = criterion_focal(outputs, labels)
            
        # Triplet Loss ê³„ì‚° (CutMixê°€ ì ìš©ë˜ì§€ ì•Šì€ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì´ìƒì ì´ë‚˜, ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ í˜„ì¬ ë°°ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ê³„ì‚°)
        hard_pairs = miner(embeddings, labels)
        loss_triplet = criterion_triplet(embeddings, labels, hard_pairs)
        
        # ìµœì¢… ì†ì‹¤ ê³„ì‚°
        loss = loss_focal + triplet_loss_weight * loss_triplet

        # ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
        train_loss += loss.item()
        # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
        _, predicted = torch.max(outputs, 1)
        
        # CutMixê°€ ì ìš©ë˜ì—ˆì„ ë•Œì™€ ì•„ë‹ ë•Œë¥¼ êµ¬ë¶„í•˜ì—¬ ì •í™•ë„ ê³„ì‚°
        if use_cutmix: # CutMixê°€ ì ìš©ëœ ê²½ìš°
            correct_predictions = lam * (predicted == targets_a.data).sum().item() + (1 - lam) * (predicted == targets_b.data).sum().item()
            train_accuracy += correct_predictions
        else: # ì¼ë°˜ì ì¸ ê²½ìš°
            train_accuracy += (predicted == labels).sum().item()

        all_preds_list.extend(predicted.detach().cpu().numpy())
        all_labels_list.extend(labels.detach().cpu().numpy())

        # tqdmì˜ ì§„í–‰ë°”ì— í‘œì‹œë  ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •
        tbar.set_description(f"Epoch [{epoch}/{num_epochs}], Train Loss: {loss.item():.4f}")

    # ì—í­ë³„ í•™ìŠµ ê²°ê³¼ ì¶œë ¥
    train_loss = train_loss / len(dataloader)
    train_accuracy = train_accuracy / len(train_dataset)
    train_f1 = f1_score(all_labels_list, all_preds_list, average='macro')


    ret = {
        "train_loss": train_loss,
        "train_accuracy": train_accuracy,
        "train_f1": train_f1,
    }

    return model, ret


def training_use_amp(model, dataloader, train_dataset, optimizer, device, epoch, num_epochs, scaler, criterions, miner, triplet_loss_weight):
    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
    train_loss = 0.0
    train_accuracy = 0
    all_preds_list = []
    all_labels_list = []
    use_cutmix = np.random.rand() < 0.5
  
    # ì†ì‹¤ í•¨ìˆ˜ë“¤ì„ ë°›ì•„ì˜µë‹ˆë‹¤.
    criterion_focal = criterions['focal']
    criterion_triplet = criterions['triplet']

    tbar = tqdm(dataloader)
    for images, labels, _ in tbar:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        
        # ğŸ”¥ autocastë¡œ float16 ì‚¬ìš©
        with autocast():
            # 50% í™•ë¥ ë¡œ CutMix ì ìš©
            if use_cutmix:
                mixed_inputs, targets_a, targets_b, lam = cutmix_data(images, labels, alpha=1.0)
                embeddings, outputs = model(mixed_inputs, labels)
            else:
                embeddings, outputs = model(images, labels)
        
            # Focal Loss ê³„ì‚°
            if use_cutmix:
                loss_focal = mixup_criterion(criterion_focal, outputs, targets_a, targets_b, lam)
            else:
                loss_focal = criterion_focal(outputs, labels)
                
            # Triplet Loss ê³„ì‚° (CutMixê°€ ì ìš©ë˜ì§€ ì•Šì€ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì´ìƒì ì´ë‚˜, ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ í˜„ì¬ ë°°ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ê³„ì‚°)
            hard_pairs = miner(embeddings, labels)
            loss_triplet = criterion_triplet(embeddings, labels, hard_pairs)
            
            # ìµœì¢… ì†ì‹¤ ê³„ì‚°
            loss = loss_focal + triplet_loss_weight * loss_triplet
        
        # âš™ï¸ AMP-aware backward + step
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()


        # ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
        train_loss += loss.item()
        # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜
        _, predicted = torch.max(outputs, 1)

        # CutMixê°€ ì ìš©ë˜ì—ˆì„ ë•Œì™€ ì•„ë‹ ë•Œë¥¼ êµ¬ë¶„í•˜ì—¬ ì •í™•ë„ ê³„ì‚°
        if use_cutmix: # CutMixê°€ ì ìš©ëœ ê²½ìš°
            correct_predictions = lam * (predicted == targets_a.data).sum().item() + (1 - lam) * (predicted == targets_b.data).sum().item()
            train_accuracy += correct_predictions
        else: # ì¼ë°˜ì ì¸ ê²½ìš°
            train_accuracy += (predicted == labels).sum().item()

        all_preds_list.extend(predicted.detach().cpu().numpy())
        all_labels_list.extend(labels.detach().cpu().numpy())

        # tqdmì˜ ì§„í–‰ë°”ì— í‘œì‹œë  ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •
        tbar.set_description(f"Epoch [{epoch}/{num_epochs}], Train Loss: {loss.item():.4f}")

    # ì—í­ë³„ í•™ìŠµ ê²°ê³¼ ì¶œë ¥
    train_loss = train_loss / len(dataloader)
    train_accuracy = train_accuracy / len(train_dataset)
    train_f1 = f1_score(all_labels_list, all_preds_list, average='macro')


    ret = {
        "train_loss": train_loss,
        "train_accuracy": train_accuracy,
        "train_f1": train_f1,
    }

    return model, ret